import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import matplotlib.pyplot as plt
import seaborn as sns
import time

# Importer les fonctions depuis le fichier model_functions.py
from model_functions import *

# Configuration de la page
st.set_page_config(
    page_title="Analyse Immobili√®re ",
    page_icon="üè†",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS personnalis√© pour am√©liorer l'apparence
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1E3A8A;
        margin-bottom: 1rem;
    }
    .sub-header {
        font-size: 1.8rem;
        color: #2563EB;
        margin-top: 2rem;
        margin-bottom: 1rem;
    }
    .section-header {
        font-size: 1.5rem;
        color: #3B82F6;
        margin-top: 1.5rem;
        margin-bottom: 0.8rem;
    }
    .highlight {
        background-color: #EFF6FF;
        padding: 20px;
        border-radius: 5px;
        border-left: 5px solid #3B82F6;
        margin-bottom: 20px;
    }
    .info-box {
        background-color: #DBEAFE;
        padding: 15px;
        border-radius: 5px;
        margin-bottom: 15px;
    }
    .stat-card {
        background-color: #F8FAFC;
        padding: 20px;
        border-radius: 10px;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        margin-bottom: 15px;
        text-align: center;
    }
    .footer {
        margin-top: 50px;
        padding-top: 20px;
        border-top: 1px solid #E5E7EB;
        text-align: center;
        font-size: 0.9rem;
        color: #6B7280;
    }
</style>
""", unsafe_allow_html=True)

# Header de l'application
st.title("üè† Analyse du March√© Immobilier Tunisien")

# Introduction et contexte
with st.expander("üìå √Ä propos de cette application", expanded=True):
    st.markdown("""
    <div class="highlight">
    <h4>Contexte</h4>
    <p>Cette application vous permet d'analyser en profondeur le march√© immobilier tunisien √† travers des donn√©es collect√©es depuis des sites de franchises immobili√®res  (Century 21, REMAX, Tecnocasa et Newkey). Que vous soyez un investisseur, un agent immobilier, ou simplement √† la recherche d'un bien, cet outil vous fournit des insights pr√©cieux sur les tendances du march√©.</p>
    
    <h4>Fonctionnalit√©s</h4>
    <ul>
        <li><strong>Analyse exploratoire</strong> : Visualisez les distributions des prix, surfaces, et autres caract√©ristiques des biens</li>
        <li><strong>Traitement des donn√©es</strong> : Nettoyez et imputez les valeurs manquantes pour une analyse plus pr√©cise</li>
        <li><strong>Mod√©lisation pr√©dictive</strong> : Utilisez des algorithmes d'apprentissage automatique pour pr√©dire les prix et identifier les facteurs d√©terminants</li>
        <li><strong>Segmentation g√©ographique</strong> : Analysez les sp√©cificit√©s du march√© par ville et quartier</li>
    </ul>
    </div>
    """, unsafe_allow_html=True)
uploaded_file = st.file_uploader("T√©l√©charger un fichier CSV", type=['csv'])


# Fonction pour nettoyer et pr√©parer les donn√©es
def preprocess_data(df):
    # Liste des colonnes num√©riques √† convertir
    numeric_columns = ['price', 'size', 'rooms', 'bedrooms', 'bathrooms', 'parkings', 
                      'construction_year', 'age', 'air_conditioning', 'central_heating', 
                      'swimming_pool', 'elevator', 'garden', 'equipped_kitchen']
    
    # Convertir chaque colonne en num√©rique
    for col in numeric_columns:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')
    
    # Convertir les dates
    if 'listing_date' in df.columns:
        try:
            df['listing_date'] = pd.to_datetime(df['listing_date'], errors='coerce')
        except:
            pass
    
    # Remplacer les valeurs potentiellement probl√©matiques
    df = df.replace(['\\N', 'N/A', 'NA', ''], np.nan)
    
    # Standardiser la casse pour les colonnes cat√©gorielles
    categorical_columns = ['property_type', 'transaction', 'city', 'state', 'neighborhood', 'finishing', 'condition']
    for col in categorical_columns:
        if col in df.columns and df[col].dtype == 'object':
            # Convertir tout en minuscules pour standardiser
            df[col] = df[col].str.lower()
    
    return df

# Fonction pour g√©n√©rer les visualisations basiques
def basic_visualizations(df):
    # Visualisation par ville
    if 'city' in df.columns:
        st.subheader("Nombre de propri√©t√©s par ville")
        city_counts = df['city'].value_counts().reset_index()
        city_counts.columns = ['ville', 'nombre']
        city_counts['ville'] = city_counts['ville'].str.title()
        
        fig = px.bar(city_counts, x='ville', y='nombre', 
                   title="Nombre de propri√©t√©s par ville")
        st.plotly_chart(fig, use_container_width=True)
    
    # Prix moyen par type de propri√©t√©
    if 'property_type' in df.columns and 'price' in df.columns:
        valid_price_df = df.dropna(subset=['price'])
        if not valid_price_df.empty:
            st.subheader("Prix moyen par type de propri√©t√©")
            price_by_type = valid_price_df.groupby('property_type')['price'].mean().reset_index()
            price_by_type.columns = ['type', 'prix_moyen']
            price_by_type['type'] = price_by_type['type'].str.capitalize()
            
            fig = px.bar(price_by_type, x='type', y='prix_moyen', 
                       title="Prix moyen par type de propri√©t√©",
                       labels={'prix_moyen': 'Prix moyen (TND)', 'type': 'Type de bien'})
            st.plotly_chart(fig, use_container_width=True)
    
    # Relation taille vs prix
    if 'size' in df.columns and 'price' in df.columns:
        valid_data = df.dropna(subset=['size', 'price'])
        if len(valid_data) > 5:
            st.subheader("Relation entre taille et prix")
            
            if 'property_type' in valid_data.columns:
                plot_data = valid_data.copy()
                plot_data['property_type_display'] = plot_data['property_type'].str.capitalize()
                
                fig = px.scatter(plot_data, x='size', y='price', 
                               color='property_type_display',
                               title="Relation entre taille et prix",
                               labels={'size': 'Surface (m¬≤)', 'price': 'Prix (TND)', 
                                     'property_type_display': 'Type de bien'})
            else:
                fig = px.scatter(valid_data, x='size', y='price', 
                               title="Relation entre taille et prix",
                               labels={'size': 'Surface (m¬≤)', 'price': 'Prix (TND)'})
            
            st.plotly_chart(fig, use_container_width=True)

# Nouvelles visualisations
def advanced_visualizations(df):
    if df is None or df.empty:
        st.warning("Aucune donn√©e disponible pour les visualisations avanc√©es.")
        return
        
    col1, col2 = st.columns(2)
    
    # Distribution des conditions
    with col1:
        if 'condition' in df.columns and not df['condition'].isna().all():
            st.subheader("Distribution des √©tats de propri√©t√©")
            condition_counts = df['condition'].value_counts().reset_index()
            condition_counts.columns = ['√©tat', 'nombre']
            condition_counts['√©tat'] = condition_counts['√©tat'].str.capitalize()
            
            fig = px.pie(condition_counts, values='nombre', names='√©tat', 
                      title="Distribution des √©tats de propri√©t√©",
                      color_discrete_sequence=px.colors.qualitative.Pastel)
            fig.update_traces(textposition='inside', textinfo='percent+label')
            st.plotly_chart(fig, use_container_width=True)
    
    # Distribution des finitions
    with col2:
        if 'finishing' in df.columns and not df['finishing'].isna().all():
            st.subheader("Niveau de finition des propri√©t√©s")
            finishing_counts = df['finishing'].value_counts().reset_index()
            finishing_counts.columns = ['finition', 'nombre']
            finishing_counts['finition'] = finishing_counts['finition'].str.capitalize()
            
            fig = px.pie(finishing_counts, values='nombre', names='finition', 
                      title="Niveau de finition des propri√©t√©s",
                      color_discrete_sequence=px.colors.qualitative.Bold)
            fig.update_traces(textposition='inside', textinfo='percent+label')
            st.plotly_chart(fig, use_container_width=True)
    
    # Visualisation des transactions par quartier
    if 'transaction' in df.columns and 'neighborhood' in df.columns and not df['neighborhood'].isna().all():
        st.subheader("Types de transaction par quartier")
        try:
            transaction_by_neighborhood = pd.crosstab(df['neighborhood'], df['transaction'])
            transaction_by_neighborhood = transaction_by_neighborhood.reset_index()
            transaction_by_neighborhood_melted = pd.melt(
                transaction_by_neighborhood, 
                id_vars=['neighborhood'], 
                var_name='transaction', 
                value_name='count'
            )
            transaction_by_neighborhood_melted['neighborhood'] = transaction_by_neighborhood_melted['neighborhood'].str.title()
            transaction_by_neighborhood_melted['transaction'] = transaction_by_neighborhood_melted['transaction'].str.capitalize()
            
            # Limiter aux 15 quartiers les plus fr√©quents pour lisibilit√©
            top_neighborhoods = df['neighborhood'].value_counts().nlargest(15).index
            filtered_data = transaction_by_neighborhood_melted[
                transaction_by_neighborhood_melted['neighborhood'].str.lower().isin(top_neighborhoods)
            ]
            
            if not filtered_data.empty:
                fig = px.bar(filtered_data, x='neighborhood', y='count', color='transaction',
                           title="Types de transaction par quartier (top 15)",
                           labels={'count': 'Nombre', 'neighborhood': 'Quartier', 'transaction': 'Type de transaction'},
                           barmode='stack')
                fig.update_layout(xaxis={'categoryorder': 'total descending'})
                st.plotly_chart(fig, use_container_width=True)
        except Exception as e:
            st.warning(f"Impossible de g√©n√©rer la visualisation des transactions par quartier: {e}")
    
    # Matrice de corr√©lation
    numeric_df = df.select_dtypes(include=['number'])
    if numeric_df.shape[1] > 2:
        st.subheader("Matrice de corr√©lation")
        
        # S√©lectionner seulement les colonnes num√©riques et supprimer les colonnes avec trop de NA
        cols_to_keep = numeric_df.columns[numeric_df.isnull().mean() < 0.5]
        if len(cols_to_keep) >= 2:  # Besoin d'au moins 2 colonnes pour une corr√©lation
            try:
                corr_df = numeric_df[cols_to_keep].corr()
                
                fig = px.imshow(corr_df, 
                               text_auto=True, 
                               aspect="auto",
                               color_continuous_scale='RdBu_r',
                               title="Corr√©lation entre les variables")
                st.plotly_chart(fig, use_container_width=True)
            except Exception as e:
                st.warning(f"Impossible de g√©n√©rer la matrice de corr√©lation: {e}")
    
    # Distribution des prix par type de propri√©t√©
    if 'price' in df.columns and 'property_type' in df.columns:
        valid_data = df.dropna(subset=['price', 'property_type'])
        if len(valid_data) > 5:
            st.subheader("Distribution des prix par type de propri√©t√©")
            
            fig = px.box(valid_data, x='property_type', y='price',
                       labels={'property_type': 'Type de propri√©t√©', 'price': 'Prix (TND)'},
                       title="Distribution des prix par type de propri√©t√©",
                       category_orders={"property_type": sorted(valid_data['property_type'].unique())})
            
            fig.update_xaxes(tickangle=45)
            st.plotly_chart(fig, use_container_width=True)

# Nouvelle section pour les imputations de donn√©es
def imputation_section(df):
    st.header("Imputation des donn√©es manquantes")
    
    if df is None or df.empty:
        st.error("Aucune donn√©e disponible pour l'imputation.")
        return df
    
    # Cr√©ation de l'interface d'imputation
    st.write("Cette section vous permet de compl√©ter les valeurs manquantes dans votre jeu de donn√©es.")
    
    try:
        # Analyser les donn√©es manquantes
        missing_data_df = analyze_missing_data(df)
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.subheader("√âtat des valeurs manquantes")
            st.dataframe(missing_data_df)
        
        with col2:
            # Graphique des valeurs manquantes
            missing_cols = missing_data_df[missing_data_df['Valeurs NA'] > 0]
            if not missing_cols.empty:
                fig = px.bar(
                    missing_cols.reset_index(), 
                    x='index', 
                    y='Pourcentage NA (%)',
                    title="Pourcentage de valeurs manquantes par colonne",
                    labels={'index': 'Colonne', 'Pourcentage NA (%)': '% manquant'}
                )
                fig.update_layout(xaxis={'categoryorder': 'total descending'})
                st.plotly_chart(fig, use_container_width=True)
            else:
                st.success("üëç Aucune valeur manquante dans vos donn√©es!")
        
        # S√©lection des colonnes √† imputer
        st.subheader("Choisir les colonnes √† imputer")
        
        # Organisation par cat√©gories
        price_cols = st.multiselect(
            "Colonnes de prix",
            [col for col in df.columns if col in ['price', 'price_ttc', 'listing_price']],
            [col for col in df.columns if col in ['price', 'price_ttc', 'listing_price'] and df[col].isna().sum() > 0]
        )
        
        condition_finishing_cols = st.multiselect(
            "Qualit√© et finition",
            [col for col in df.columns if col in ['condition', 'finishing']],
            [col for col in df.columns if col in ['condition', 'finishing'] and df[col].isna().sum() > 0]
        )
        
        age_year_cols = st.multiselect(
            "√Çge et ann√©e de construction",
            [col for col in df.columns if col in ['age', 'construction_year']],
            [col for col in df.columns if col in ['age', 'construction_year'] and df[col].isna().sum() > 0]
        )
        
        room_cols = st.multiselect(
            "Pi√®ces, chambres, salles de bain, etc.",
            [col for col in df.columns if col in ['rooms', 'bedrooms', 'bathrooms', 'parkings']],
            [col for col in df.columns if col in ['rooms', 'bedrooms', 'bathrooms', 'parkings'] and df[col].isna().sum() > 0]
        )
        
        binary_cols = st.multiselect(
            "√âquipements (variables binaires)",
            [col for col in df.columns if df[col].nunique() <= 2 and col not in ['transaction', 'city', 'property_type', 'neighborhood']],
            [col for col in df.columns if df[col].nunique() <= 2 and df[col].isna().sum() > 0 and col not in ['transaction', 'city', 'property_type', 'neighborhood']]
        )
        
        # Bouton pour lancer l'imputation
        impute_button = st.button("Imputer les valeurs manquantes", type="primary")
        
        if impute_button:
            if df is None or df.empty:
                st.error("Aucune donn√©e disponible pour l'imputation.")
                return df
                
            # Cr√©er une copie pour l'imputation
            try:
                df_imputed = df.copy()
                progress_placeholder = st.empty()
                
                with st.spinner("Imputation en cours..."):
                    # Imputation progressive avec barre de progression
                    progress_bar = st.progress(0)
                    
                    # 1. Imputation des prix
                    # 
                    
                    # 2. Imputation de la condition
                    if 'condition' in condition_finishing_cols:
                        progress_placeholder.write("Imputation de la condition...")
                        try:
                            df_imputed = impute_condition_simple(df_imputed)
                            progress_bar.progress(40)
                            time.sleep(0.5)
                        except Exception as e:
                            st.error(f"Erreur lors de l'imputation de la condition: {e}")
                    
                    # 3. Imputation de la finition
                    if 'finishing' in condition_finishing_cols:
                        progress_placeholder.write("Imputation du niveau de finition...")
                        try:
                            df_imputed = impute_finishing_simple(df_imputed)
                            progress_bar.progress(60)
                            time.sleep(0.5)
                        except Exception as e:
                            st.error(f"Erreur lors de l'imputation de la finition: {e}")
                    
                    # 4. Imputation de l'√¢ge et ann√©e de construction
                    if age_year_cols:
                        progress_placeholder.write("Imputation de l'√¢ge et ann√©e de construction...")
                        try:
                            df_imputed = impute_property_year_age(
                                df_imputed, 
                                impute_year='construction_year' in age_year_cols, 
                                impute_age='age' in age_year_cols
                            )
                            df_imputed['construction_year']=2025-df_imputed['age']
                            progress_bar.progress(75)
                            time.sleep(0.5)
                        except Exception as e:
                            st.error(f"Erreur lors de l'imputation de l'√¢ge/ann√©e: {e}")
                    
                    # 5. Imputation des caract√©ristiques binaires
                    if binary_cols:
                        progress_placeholder.write("Imputation des √©quipements...")
                        try:
                            df_imputed = impute_binary_amenities(df_imputed, binary_columns=binary_cols)
                            progress_bar.progress(85)
                            time.sleep(0.5)
                        except Exception as e:
                            st.error(f"Erreur lors de l'imputation des √©quipements: {e}")
                    
                    # 6. Imputation des pi√®ces et caract√©ristiques
                    for room_col in room_cols:
                        progress_placeholder.write(f"Imputation de {room_col}...")
                        try:
                            df_imputed = simple_impute_rooms(df_imputed, rooms_col=room_col)
                            time.sleep(0.3)
                        except Exception as e:
                            st.error(f"Erreur lors de l'imputation de {room_col}: {e}")
                    # imputation des prix
                    if price_cols:
                        progress_placeholder.write("Imputation des prix...")
                        try:
                            # df_imputed = impute_missing_prices(df_imputed)
                            df_imputed['price'] = df_imputed.groupby(['neighborhood', 'property_type','transaction'])['price'].transform(lambda x: x.fillna(x.mean()))
                            df_imputed['price_ttc'] = df_imputed.groupby(['neighborhood', 'property_type','transaction'])['price_ttc'].transform(lambda x: x.fillna(x.mean()))
                            df_imputed['price'] = df.groupby(['city','transaction'])['price'].transform(lambda x: x.fillna(x.mean()))
                            df_imputed['price_ttc'] = df.groupby(['city','transaction'])['price_ttc'].transform(lambda x: x.fillna(x.mean()))
                            df_imputed = df_imputed[df_imputed['price'].notnull()]
    
                            df_imputed['suffix'] = df_imputed['suffix'].fillna('TTC')
                            
                            df_imputed['listing_price'] = df_imputed['listing_price'].fillna(df_imputed['price'])
                            
                            progress_bar.progress(100)
                            time.sleep(0.5)  
                        except Exception as e:
                            st.error(f"Erreur lors de l'imputation des prix: {e}")
                    progress_bar.progress(100)
                    progress_placeholder.empty()
                
                # Comparer l'avant/apr√®s
                st.subheader("R√©sultats de l'imputation")
                
                col1, col2 = st.columns(2)
                
                # Analyse des valeurs manquantes avant
                with col1:
                    df.drop(columns=['amenities'], inplace=True)
                    st.write("Avant imputation")
                    missing_before = analyze_missing_data(df)
                    st.dataframe(missing_before)
                    
                    # Pourcentage global des donn√©es manquantes avant
                    total_elements = df.shape[0] * df.shape[1]
                    total_missing = df.isna().sum().sum()
                    pct_missing_before = (total_missing / total_elements) * 100
                    
                    st.metric(
                        "Pourcentage global de donn√©es manquantes",
                        f"{pct_missing_before:.2f}%"
                    )
                
                # Analyse des valeurs manquantes apr√®s
                with col2:
                    st.write("Apr√®s imputation")
                    df_imputed.drop(columns=['amenities'], inplace=True)
                    missing_after = analyze_missing_data(df_imputed)
                    st.dataframe(missing_after)
                    
                    # Pourcentage global des donn√©es manquantes apr√®s
                    total_missing_after = df_imputed.isna().sum().sum()
                    pct_missing_after = (total_missing_after / total_elements) * 100
                    
                    st.metric(
                        "Pourcentage global de donn√©es manquantes",
                        f"{pct_missing_after:.2f}%",
                        f"-{pct_missing_before - pct_missing_after:.2f}%"
                    )
                
                # Visualisation de l'impact de l'imputation
                st.subheader("Visualisation de l'impact de l'imputation")
                
                # S√©lectionnez une colonne pour visualiser l'impact de l'imputation
                all_imputed_cols = price_cols + condition_finishing_cols + age_year_cols + room_cols + binary_cols
                
                if True:
                    vis_col = st.selectbox(
                        "S√©lectionner une colonne pour visualiser l'impact de l'imputation",
                        all_imputed_cols
                    )
                    
                    if vis_col in df_imputed.columns:
                        col1, col2 = st.columns(2)
                        
                        with col1:
                            st.write(f"Distribution de {vis_col} avant imputation")
                            
                            if pd.api.types.is_numeric_dtype(df[vis_col]):
                                # Histogramme pour les donn√©es num√©riques
                                fig = px.histogram(
                                    df.dropna(subset=[vis_col]), 
                                    x=vis_col,
                                    title=f"Distribution de {vis_col} avant imputation",
                                    nbins=30,
                                    opacity=0.7
                                )
                                st.plotly_chart(fig, use_container_width=True)
                            else:
                                # Barres pour les donn√©es cat√©gorielles
                                value_counts = df[vis_col].value_counts().reset_index()
                                value_counts.columns = ['valeur', 'nombre']
                                
                                fig = px.bar(
                                    value_counts,
                                    x='valeur',
                                    y='nombre',
                                    title=f"Distribution de {vis_col} avant imputation"
                                )
                                st.plotly_chart(fig, use_container_width=True)
                        
                        with col2:
                            st.write(f"Distribution de {vis_col} apr√®s imputation")
                            
                            if pd.api.types.is_numeric_dtype(df_imputed[vis_col]):
                                # Histogramme pour les donn√©es num√©riques
                                fig = px.histogram(
                                    df_imputed, 
                                    x=vis_col,
                                    title=f"Distribution de {vis_col} apr√®s imputation",
                                    nbins=30,
                                    opacity=0.7
                                )
                                # Ajouter une ligne pour marquer les valeurs imput√©es
                                orig_values = df[~df[vis_col].isna()][vis_col]
                                fig.add_traces(
                                    px.histogram(
                                        orig_values, 
                                        x=orig_values,
                                        nbins=30,
                                        opacity=0.7
                                    ).data
                                )
                                fig.data[0].marker.color = 'blue'  # Toutes les valeurs
                                fig.data[1].marker.color = 'lightgreen'  # Valeurs originales
                                fig.data[0].name = 'Toutes les valeurs (incluant imput√©es)'
                                fig.data[1].name = 'Valeurs originales uniquement'
                                fig.update_layout(barmode='overlay', legend=dict(orientation='h'))
                                
                                st.plotly_chart(fig, use_container_width=True)
                            else:
                                # Barres pour les donn√©es cat√©gorielles
                                value_counts = df_imputed[vis_col].value_counts().reset_index()
                                value_counts.columns = ['valeur', 'nombre']
                                
                                fig = px.bar(
                                    value_counts,
                                    x='valeur',
                                    y='nombre',
                                    title=f"Distribution de {vis_col} apr√®s imputation"
                                )
                                st.plotly_chart(fig, use_container_width=True)
                
                # Option pour continuer avec les donn√©es imput√©es
                if st.button("Utiliser les donn√©es imput√©es pour la suite de l'analyse"):
                    st.session_state['df_imputed'] = df_imputed
                    st.success("‚úÖ Les donn√©es imput√©es sont maintenant utilis√©es pour l'analyse!")
                    st.experimental_rerun()  # R√©ex√©cuter l'application pour utiliser les donn√©es imput√©es
                    
                return df_imputed  # Retourner les donn√©es imput√©es
                
            except Exception as e:
                st.error(f"Erreur lors de l'imputation: {e}")
                st.info("Conseil: V√©rifiez les donn√©es et essayez √† nouveau.")
                return df
    except Exception as e:
        st.error(f"Erreur lors de l'analyse des donn√©es manquantes: {e}")
        return df
        
    return df

# Nouvelle section pour l'apprentissage supervis√©
def supervised_learning_section(df, filtered_df):
    st.header("Mod√®les d'Apprentissage Supervis√©")
    
    if df is None or filtered_df is None or df.empty or filtered_df.empty:
        st.error("Aucune donn√©e disponible pour l'apprentissage supervis√©.")
        return
    
    # Pr√©parer les donn√©es pour la r√©gression
    with st.spinner("Pr√©paration des donn√©es pour l'apprentissage..."):
        try:
            df_prep = prepare_data_for_regression(filtered_df)
            st.success("Donn√©es pr√©par√©es pour l'apprentissage supervis√© !")
        except Exception as e:
            st.error(f"Erreur lors de la pr√©paration des donn√©es : {e}")
            return
    
    # Param√®tres pour les mod√®les
    st.subheader("Param√®tres du mod√®le")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if 'city' in filtered_df.columns:
            city_options = ["Toutes"] + sorted(filtered_df['city'].dropna().unique().tolist())
            selected_city = st.selectbox("Ville pour le mod√®le", city_options)
            selected_city = None if selected_city == "Toutes" else selected_city
        else:
            selected_city = None
            st.write("Information sur la ville non disponible")
    
    with col2:
        if 'property_type' in filtered_df.columns:
            property_options = ["Tous"] + sorted(filtered_df['property_type'].dropna().unique().tolist())
            selected_property = st.selectbox("Type de propri√©t√© pour le mod√®le", property_options)
            selected_property = None if selected_property == "Tous" else selected_property
        else:
            selected_property = None
            st.write("Information sur le type de propri√©t√© non disponible")
    
    with col3:
        if 'transaction' in filtered_df.columns:
            transaction_options = ["Toutes"] + sorted(filtered_df['transaction'].dropna().unique().tolist())
            selected_transaction = st.selectbox("Type de transaction pour le mod√®le", transaction_options)
            selected_transaction = None if selected_transaction == "Toutes" else selected_transaction
        else:
            selected_transaction = None
            st.write("Information sur le type de transaction non disponible")
    
    # S√©lection du mod√®le
    model_type = st.selectbox(
        "S√©lectionner le mod√®le",
        ["Comparaison de mod√®les", "R√©gression Lin√©aire", "Random Forest", "XGBoost"]
    )
    
    # Bouton pour lancer l'entra√Ænement
    if st.button("Entra√Æner le mod√®le"):
        with st.spinner("Entra√Ænement du mod√®le en cours..."):
            # V√©rifier qu'il y a assez de donn√©es
            if len(df_prep) < 10:
                st.error("Pas assez de donn√©es pour l'entra√Ænement du mod√®le. Veuillez √©largir les crit√®res de s√©lection.")
            else:
                try:
                    # Cr√©er un conteneur pour les r√©sultats
                    results_container = st.container()
                    
                    with results_container:
                        st.subheader(f"R√©sultats pour {model_type}")
                        
                        # Ex√©cuter le mod√®le s√©lectionn√©
                        if model_type == "Comparaison de mod√®les":
                            comparison = comparer_modeles(
                                df_prep, 
                                city=selected_city, 
                                property_type=selected_property, 
                                transaction=selected_transaction
                            )
                            st.dataframe(comparison)
                            
                            try:
                                # Convertir les figures Matplotlib en Plotly pour Streamlit
                                st.pyplot(plt.gcf())  # R√©cup√®re la figure actuelle (courante)
                            except Exception as e:
                                st.warning(f"Impossible d'afficher le graphique: {e}")
                        
                        elif model_type == "R√©gression Lin√©aire":
                            model, importance, metrics = regression_par_segment(
                                df_prep, 
                                city=selected_city, 
                                property_type=selected_property, 
                                transaction=selected_transaction
                            )
                            
                            # Afficher les m√©triques
                            st.write(f"R¬≤ (test): {metrics['test_r2']:.4f}")
                            st.write(f"RMSE (test): {metrics['test_rmse']:.2f}")
                            st.write(f"MAE (test): {metrics['test_mae']:.2f}")
                            
                            # Afficher l'importance des caract√©ristiques
                            st.subheader("Importance des caract√©ristiques")
                            
                            # Cr√©er un graphique Plotly pour l'importance des caract√©ristiques
                            top_features = importance.head(10)
                            fig = px.bar(
                                top_features,
                                x='Coefficient',
                                y='Caract√©ristique',
                                orientation='h',
                                title="Top 10 des caract√©ristiques les plus importantes",
                                color='Coefficient',
                                color_continuous_scale=px.colors.diverging.RdBu,
                                color_continuous_midpoint=0
                            )
                            st.plotly_chart(fig, use_container_width=True)
                            
                            try:
                                # Convertir les figures Matplotlib en Plotly pour Streamlit
                                st.pyplot(plt.gcf())
                            except Exception as e:
                                st.warning(f"Impossible d'afficher le graphique: {e}")
                        
                        elif model_type == "Random Forest":
                            model, importance, metrics = random_forest_par_segment(
                                df_prep, 
                                city=selected_city, 
                                property_type=selected_property, 
                                transaction=selected_transaction
                            )
                            
                            # Afficher les m√©triques
                            st.write(f"R¬≤ (test): {metrics['test_r2']:.4f}")
                            st.write(f"RMSE (test): {metrics['test_rmse']:.2f}")
                            st.write(f"MAE (test): {metrics['test_mae']:.2f}")
                            
                            # Afficher l'importance des caract√©ristiques
                            st.subheader("Importance des caract√©ristiques")
                            
                            # Cr√©er un graphique Plotly pour l'importance des caract√©ristiques
                            top_features = importance.head(10)
                            fig = px.bar(
                                top_features,
                                x='Importance',
                                y='Caract√©ristique',
                                orientation='h',
                                title="Top 10 des caract√©ristiques les plus importantes",
                                color='Importance',
                                color_continuous_scale='Viridis'
                            )
                            st.plotly_chart(fig, use_container_width=True)
                            
                            try:
                                # Convertir les figures Matplotlib en Plotly pour Streamlit
                                st.pyplot(plt.gcf())
                            except Exception as e:
                                st.warning(f"Impossible d'afficher le graphique: {e}")
                        
                        elif model_type == "XGBoost":
                            model, importance, test_r2 = xgboost_simple(
                                df_prep, 
                                city=selected_city, 
                                property_type=selected_property, 
                                transaction=selected_transaction
                            )
                            
                            # Afficher les m√©triques
                            st.write(f"R¬≤ (test): {test_r2:.4f}")
                            
                            # Afficher l'importance des caract√©ristiques
                            st.subheader("Importance des caract√©ristiques")
                            
                            # Cr√©er un graphique Plotly pour l'importance des caract√©ristiques
                            top_features = importance.head(10)
                            fig = px.bar(
                                top_features,
                                x='Importance',
                                y='Caract√©ristique',
                                orientation='h',
                                title="Top 10 des caract√©ristiques les plus importantes",
                                color='Importance',
                                color_continuous_scale='Viridis'
                            )
                            st.plotly_chart(fig, use_container_width=True)
                            
                            try:
                                # Convertir les figures Matplotlib en Plotly pour Streamlit
                                st.pyplot(plt.gcf())
                            except Exception as e:
                                st.warning(f"Impossible d'afficher le graphique: {e}")
                            
                except Exception as e:
                    st.error(f"Une erreur s'est produite lors de l'entra√Ænement du mod√®le: {e}")

def unsupervised_learning_section(df, filtered_df):
    st.header("ü§ñ Apprentissage Non Supervis√© - Clustering")
    
    if df is None or filtered_df is None or df.empty or filtered_df.empty:
        st.error("Aucune donn√©e disponible pour l'apprentissage non supervis√©.")
        return
    
    st.markdown("""
    <div class="info-box">
    L'apprentissage non supervis√© permet de d√©couvrir des structures cach√©es dans les donn√©es sans avoir de variable cible.
    Nous utiliserons trois algorithmes de clustering : K-Means, DBSCAN et CAH (Classification Ascendante Hi√©rarchique).
    </div>
    """, unsafe_allow_html=True)
    
    # ============================================
    # SECTION 1: FILTRES ET S√âLECTION DES DONN√âES
    # ============================================
    
    st.subheader("üîç Filtres et S√©lection des Donn√©es")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        # Filtre par ville
        if 'city' in filtered_df.columns:
            city_options = ["Toutes"] + sorted(filtered_df['city'].dropna().unique().tolist())
            selected_city = st.selectbox("Ville", city_options, key="clustering_city")
            selected_city = None if selected_city == "Toutes" else selected_city
        else:
            selected_city = None
            st.info("Information sur la ville non disponible")
    
    with col2:
        # Filtre par type de propri√©t√©
        if 'property_type' in filtered_df.columns:
            property_options = ["Tous"] + sorted(filtered_df['property_type'].dropna().unique().tolist())
            selected_property = st.selectbox("Type de propri√©t√©", property_options, key="clustering_property")
            selected_property = None if selected_property == "Tous" else selected_property
        else:
            selected_property = None
            st.info("Information sur le type de propri√©t√© non disponible")
    
    with col3:
        # Filtre par type de transaction
        if 'transaction' in filtered_df.columns:
            transaction_options = ["Toutes"] + sorted(filtered_df['transaction'].dropna().unique().tolist())
            selected_transaction = st.selectbox("Type de transaction", transaction_options, key="clustering_transaction")
            selected_transaction = None if selected_transaction == "Toutes" else selected_transaction
        else:
            selected_transaction = None
            st.info("Information sur le type de transaction non disponible")
    
    # Appliquer les filtres
    df_for_clustering = filtered_df.copy()
    filter_applied = False
    
    if selected_city is not None:
        df_for_clustering = df_for_clustering[df_for_clustering['city'] == selected_city]
        filter_applied = True
    if selected_property is not None:
        df_for_clustering = df_for_clustering[df_for_clustering['property_type'] == selected_property]
        filter_applied = True
    if selected_transaction is not None:
        df_for_clustering = df_for_clustering[df_for_clustering['transaction'] == selected_transaction]
        filter_applied = True
    
    # Afficher les informations sur le filtrage
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Observations avant filtrage", len(filtered_df))
    with col2:
        st.metric("Observations apr√®s filtrage", len(df_for_clustering))
    with col3:
        reduction_pct = ((len(filtered_df) - len(df_for_clustering)) / len(filtered_df)) * 100 if len(filtered_df) > 0 else 0
        st.metric("R√©duction", f"-{reduction_pct:.1f}%")
    
    # V√©rifier qu'on a assez de donn√©es
    if len(df_for_clustering) < 10:
        st.warning("‚ö†Ô∏è Pas assez de donn√©es pour l'analyse de clustering (minimum 10 observations). Veuillez √©largir les filtres.")
        return
    
    # ============================================
    # SECTION 2: S√âLECTION DES CARACT√âRISTIQUES
    # ============================================
    
    st.subheader("üìä S√©lection des Caract√©ristiques")
    
    # Obtenir les colonnes num√©riques disponibles
    numeric_cols = df_for_clustering.select_dtypes(include=['number']).columns.tolist()
    exclude_cols = ['date', 'source', 'neighborhood', 'suffix', 'listing_price', 'price_ttc', 'construction_year']
    available_features = [col for col in numeric_cols if col not in exclude_cols]
    
    if not available_features:
        st.error("‚ùå Aucune caract√©ristique num√©rique disponible pour le clustering.")
        return
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        selected_features = st.multiselect(
            "S√©lectionner les caract√©ristiques pour le clustering",
            available_features,
            default=available_features[:6] if len(available_features) >= 6 else available_features,
            help="Choisissez les caract√©ristiques qui seront utilis√©es pour identifier les groupes de propri√©t√©s similaires"
        )
    
    with col2:
        st.write("**Caract√©ristiques disponibles:**")
        st.write(f"‚Ä¢ Total: {len(available_features)}")
        st.write(f"‚Ä¢ S√©lectionn√©es: {len(selected_features)}")
        if selected_features:
            st.write("**Liste s√©lectionn√©e:**")
            for feature in selected_features:
                st.write(f"- {feature}")
    
    if not selected_features:
        st.warning("‚ö†Ô∏è Veuillez s√©lectionner au moins une caract√©ristique pour continuer.")
        return
    
    # ============================================
    # SECTION 3: PARAM√àTRES DES ALGORITHMES
    # ============================================
    
    st.subheader("‚öôÔ∏è Configuration des Algorithmes")
    
    col1, col2 = st.columns(2)
    
    with col1:
        algorithm = st.selectbox(
            "Algorithme de clustering",
            ["K-Means", "DBSCAN", "CAH (Classification Ascendante Hi√©rarchique)", "Comparaison des 3 m√©thodes"],
            help="Choisissez l'algorithme de clustering √† utiliser"
        )
    
    with col2:
        n_components_pca = st.slider(
            "Nombre de composantes PCA pour visualisation",
            min_value=2,
            max_value=min(len(selected_features), 10),
            value=min(3, len(selected_features)),
            help="Nombre de composantes principales √† conserver pour la visualisation"
        )
    
    # Param√®tres sp√©cifiques selon l'algorithme
    st.write("**Param√®tres sp√©cifiques:**")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        # Param√®tres K-Means
        if algorithm in ["K-Means", "Comparaison des 3 m√©thodes"]:
            st.write("üîµ **K-Means**")
            max_clusters = min(10, len(df_for_clustering) // 5)
            n_clusters_range = st.slider(
                "Nombre de clusters √† tester",
                min_value=2,
                max_value=max_clusters,
                value=(2, min(8, max_clusters)),
                key="kmeans_range"
            )
    
    with col2:
        # Param√®tres DBSCAN
        if algorithm in ["DBSCAN", "Comparaison des 3 m√©thodes"]:
            st.write("üî¥ **DBSCAN**")
            eps_range = st.slider(
                "Range eps",
                min_value=0.1,
                max_value=5.0,
                value=(0.3, 2.0),
                step=0.1,
                key="dbscan_eps"
            )
            min_samples_range = st.slider(
                "Range min_samples",
                min_value=2,
                max_value=20,
                value=(3, 10),
                key="dbscan_samples"
            )
    
    with col3:
        # Param√®tres CAH
        if algorithm in ["CAH (Classification Ascendante Hi√©rarchique)", "Comparaison des 3 m√©thodes"]:
            st.write("üü¢ **CAH**")
            linkage_method = st.selectbox(
                "M√©thode de liaison",
                ["ward", "complete", "average", "single"],
                index=0,
                key="cah_linkage"
            )
            max_clusters_cah = st.slider(
                "Nombre max de clusters CAH",
                min_value=2,
                max_value=min(15, len(df_for_clustering) // 3),
                value=min(10, len(df_for_clustering) // 3),
                key="cah_max"
            )
    
    # ============================================
    # SECTION 4: LANCEMENT DE L'ANALYSE
    # ============================================
    
    if st.button("üöÄ Lancer l'Analyse de Clustering", type="primary"):
        with st.spinner("üîÑ Pr√©paration des donn√©es..."):
            try:
                # Pr√©parer les donn√©es pour le clustering
                df_scaled, scaler, feature_names = prepare_data_for_clustering(
                    df_for_clustering, 
                    features_for_clustering=selected_features
                )
                
                if len(df_scaled) < 10:
                    st.error("‚ùå Pas assez de donn√©es valides apr√®s nettoyage. V√©rifiez vos donn√©es.")
                    return
                
                st.success(f"‚úÖ Donn√©es pr√©par√©es: {len(df_scaled)} observations avec {len(feature_names)} caract√©ristiques")
                
                # Afficher les caract√©ristiques utilis√©es
                st.info(f"**Caract√©ristiques utilis√©es:** {', '.join(feature_names)}")
                
                # Appliquer PCA pour la visualisation
                with st.spinner("üîÑ Application de l'analyse PCA..."):
                    pca_model, df_pca, explained_variance = apply_pca_analysis(df_scaled, n_components_pca)
                
                st.write(f"**Variance expliqu√©e par PCA:** {explained_variance.sum()*100:.1f}% (composantes 1-{n_components_pca})")
                
                # ============================================
                # EX√âCUTION DES ALGORITHMES
                # ============================================
                
                if algorithm == "K-Means":
                    st.subheader("üîµ R√©sultats K-Means")
                    
                    with st.spinner("üîÑ Ex√©cution de K-Means..."):
                        kmeans_model, best_n_clusters, cluster_labels, metrics, scores, n_clusters_list = apply_kmeans_clustering(
                            df_scaled, 
                            n_clusters_range=n_clusters_range,
                            random_state=42
                        )
                    
                    # Afficher les m√©triques
                    col1, col2, col3, col4 = st.columns(4)
                    with col1:
                        st.metric("üéØ Clusters optimaux", best_n_clusters)
                    with col2:
                        st.metric("üìä Score Silhouette", f"{metrics['silhouette_score']:.4f}")
                    with col3:
                        st.metric("üìà Score Calinski-Harabasz", f"{metrics['calinski_harabasz_score']:.0f}")
                    with col4:
                        st.metric("‚ö° Inertie", f"{metrics['inertia']:.0f}")
                    
                    # Graphique d'optimisation
                    fig_optimization = px.line(
                        x=n_clusters_list, 
                        y=scores,
                        title="Optimisation du nombre de clusters - Score de Silhouette",
                        labels={'x': 'Nombre de clusters', 'y': 'Score de Silhouette'},
                        markers=True
                    )
                    fig_optimization.add_vline(
                        x=best_n_clusters, 
                        line_dash="dash", 
                        line_color="red", 
                        annotation_text=f"Optimal: {best_n_clusters}"
                    )
                    fig_optimization.update_layout(hovermode="x unified")
                    st.plotly_chart(fig_optimization, use_container_width=True)
                    
                    # Visualisation avec matplotlib
                    st.subheader("üìä Visualisations K-Means")
                    try:
                        # Utiliser le code corrig√© avec matplotlib
                        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
                        
                        # 1. Clusters en 2D PCA
                        colors = plt.cm.Set3(np.linspace(0, 1, len(set(cluster_labels))))
                        for i, label in enumerate(sorted(set(cluster_labels))):
                            mask = cluster_labels == label
                            axes[0,0].scatter(df_pca.iloc[mask, 0], df_pca.iloc[mask, 1], 
                                             c=[colors[i]], label=f'Cluster {label}', alpha=0.7)
                        axes[0,0].set_xlabel('PC1')
                        axes[0,0].set_ylabel('PC2')
                        axes[0,0].set_title('K-Means - Clusters (2D PCA)')
                        axes[0,0].legend()
                        axes[0,0].grid(True, alpha=0.3)
                        
                        # 2. Distribution des clusters
                        cluster_counts = pd.Series(cluster_labels).value_counts().sort_index()
                        bars = axes[0,1].bar(cluster_counts.index, cluster_counts.values, color=colors[:len(cluster_counts)])
                        axes[0,1].set_xlabel('Cluster')
                        axes[0,1].set_ylabel('Nombre de propri√©t√©s')
                        axes[0,1].set_title('Distribution des clusters K-Means')
                        axes[0,1].grid(True, alpha=0.3)
                        
                        # Ajouter les valeurs sur les barres
                        for bar, count in zip(bars, cluster_counts.values):
                            axes[0,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, 
                                           str(count), ha='center', va='bottom')
                        
                        # 3. √âvolution du score de silhouette
                        axes[1,0].plot(n_clusters_list, scores, 'o-', linewidth=2, markersize=8)
                        axes[1,0].axvline(x=best_n_clusters, color='red', linestyle='--', 
                                          label=f'Optimal: {best_n_clusters} clusters')
                        axes[1,0].set_xlabel('Nombre de clusters')
                        axes[1,0].set_ylabel('Score de silhouette')
                        axes[1,0].set_title('Optimisation du nombre de clusters')
                        axes[1,0].legend()
                        axes[1,0].grid(True, alpha=0.3)
                        
                        # 4. Variance expliqu√©e par PCA
                        pc_names = [f'PC{i+1}' for i in range(len(explained_variance))]
                        axes[1,1].bar(pc_names, explained_variance * 100, color='lightblue', edgecolor='black')
                        axes[1,1].set_xlabel('Composantes principales')
                        axes[1,1].set_ylabel('Variance expliqu√©e (%)')
                        axes[1,1].set_title('Variance expliqu√©e par PCA')
                        axes[1,1].grid(True, alpha=0.3)
                        
                        # Ajouter les pourcentages sur les barres
                        for i, var in enumerate(explained_variance):
                            axes[1,1].text(i, var * 100 + 1, f'{var*100:.1f}%', ha='center', va='bottom')
                        
                        plt.tight_layout()
                        st.pyplot(fig)
                        
                        # Analyser les clusters
                        analyze_kmeans_clusters(df_for_clustering, df_scaled, cluster_labels, feature_names)
                        
                    except Exception as e:
                        st.error(f"Erreur lors de la visualisation: {e}")
                
                elif algorithm == "DBSCAN":
                    st.subheader("üî¥ R√©sultats DBSCAN")
                    
                    with st.spinner("üîÑ Ex√©cution de DBSCAN..."):
                        dbscan_model, cluster_labels, metrics = apply_dbscan_clustering(
                            df_scaled,
                            eps_range=eps_range,
                            min_samples_range=min_samples_range
                        )
                    
                    # Afficher les m√©triques
                    col1, col2, col3, col4 = st.columns(4)
                    with col1:
                        st.metric("üéØ Clusters trouv√©s", metrics['n_clusters'])
                    with col2:
                        st.metric("üîç Points de bruit", metrics['n_noise_points'])
                    with col3:
                        st.metric("üìä Ratio de bruit", f"{metrics['noise_ratio']*100:.1f}%")
                    with col4:
                        if metrics['silhouette_score'] > 0:
                            st.metric("üìà Score Silhouette", f"{metrics['silhouette_score']:.4f}")
                        else:
                            st.metric("üìà Score Silhouette", "N/A")
                    
                    # Afficher les param√®tres optimaux
                    st.info(f"**Param√®tres optimaux:** eps={metrics['eps']:.3f}, min_samples={metrics['min_samples']}")
                    
                    # Visualisation DBSCAN
                    st.subheader("üìä Visualisations DBSCAN")
                    try:
                        visualize_dbscan_results(df_pca, cluster_labels, explained_variance)
                        
                        # Analyser les clusters DBSCAN
                        analyze_dbscan_clusters(df_for_clustering, df_scaled, cluster_labels, feature_names, metrics)
                        
                    except Exception as e:
                        st.error(f"Erreur lors de la visualisation DBSCAN: {e}")
                
                elif algorithm == "CAH (Classification Ascendante Hi√©rarchique)":
                    st.subheader("üü¢ R√©sultats CAH")
                    
                    with st.spinner("üîÑ Ex√©cution de CAH..."):
                        linkage_matrix, cluster_labels, optimal_n_clusters, cah_metrics = apply_cah_clustering(
                            df_scaled,
                            max_clusters=max_clusters_cah,
                            linkage_method=linkage_method
                        )
                    
                    # Afficher les m√©triques
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("üéØ Clusters optimaux", optimal_n_clusters)
                    with col2:
                        st.metric("üìä Score Silhouette", f"{cah_metrics['silhouette_score']:.4f}")
                    with col3:
                        st.metric("üîó M√©thode de liaison", linkage_method)
                    
                    # Dendrogramme
                    st.subheader("üå≥ Dendrogramme")
                    try:
                        dendrogram_data = visualize_cah_dendrogram(
                            linkage_matrix,
                            optimal_n_clusters=optimal_n_clusters,
                            max_display=min(50, len(df_scaled))
                        )
                        st.pyplot(plt.gcf())
                        
                        # Analyse compl√®te CAH
                        results_df, detailed_df, hierarchy_df = complete_cah_analysis_after_dendrogram(
                            df, df_scaled, linkage_matrix, cluster_labels, optimal_n_clusters, feature_names
                        )
                        
                        # Afficher les r√©sultats sous forme de tableaux
                        st.subheader("üìã Analyse des Clusters CAH")
                        st.dataframe(results_df, use_container_width=True)
                        
                        with st.expander("üîç D√©tail de toutes les propri√©t√©s par cluster"):
                            st.dataframe(detailed_df, use_container_width=True)
                        
                    except Exception as e:
                        st.error(f"Erreur lors de l'analyse CAH: {e}")
                
                else:  # Comparaison des 3 m√©thodes
                    st.subheader("üîÑ Comparaison des 3 M√©thodes")
                    
                    results = {}
                    
                    # K-Means
                    with st.spinner("üîÑ Ex√©cution de K-Means..."):
                        kmeans_model, kmeans_n_clusters, kmeans_labels, kmeans_metrics, _, _ = apply_kmeans_clustering(
                            df_scaled, n_clusters_range=n_clusters_range, random_state=42
                        )
                        results['K-Means'] = {
                            'labels': kmeans_labels,
                            'n_clusters': kmeans_n_clusters,
                            'silhouette_score': kmeans_metrics['silhouette_score'],
                            'method': 'K-Means'
                        }
                    
                    # DBSCAN
                    with st.spinner("üîÑ Ex√©cution de DBSCAN..."):
                        dbscan_model, dbscan_labels, dbscan_metrics = apply_dbscan_clustering(
                            df_scaled, eps_range=eps_range, min_samples_range=min_samples_range
                        )
                        results['DBSCAN'] = {
                            'labels': dbscan_labels,
                            'n_clusters': dbscan_metrics['n_clusters'],
                            'silhouette_score': dbscan_metrics['silhouette_score'],
                            'noise_points': dbscan_metrics['n_noise_points'],
                            'method': 'DBSCAN'
                        }
                    
                    # CAH
                    with st.spinner("üîÑ Ex√©cution de CAH..."):
                        linkage_matrix, cah_labels, cah_n_clusters, cah_metrics = apply_cah_clustering(
                            df_scaled, max_clusters=max_clusters_cah, linkage_method=linkage_method
                        )
                        results['CAH'] = {
                            'labels': cah_labels,
                            'n_clusters': cah_n_clusters,
                            'silhouette_score': cah_metrics['silhouette_score'],
                            'method': 'CAH'
                        }
                    
                    # Tableau de comparaison
                    comparison_data = {
                        'M√©thode': ['K-Means', 'DBSCAN', 'CAH'],
                        'Nombre de clusters': [
                            results['K-Means']['n_clusters'],
                            results['DBSCAN']['n_clusters'],
                            results['CAH']['n_clusters']
                        ],
                        'Score Silhouette': [
                            f"{results['K-Means']['silhouette_score']:.4f}",
                            f"{results['DBSCAN']['silhouette_score']:.4f}" if results['DBSCAN']['silhouette_score'] > 0 else "N/A",
                            f"{results['CAH']['silhouette_score']:.4f}"
                        ],
                        'Points de bruit': [
                            "0",
                            str(results['DBSCAN']['noise_points']),
                            "0"
                        ]
                    }
                    
                    comparison_df = pd.DataFrame(comparison_data)
                    st.dataframe(comparison_df, use_container_width=True)
                    
                    # Visualisations comparatives
                    st.subheader("üìä Visualisations Comparatives")
                    
                    fig, axes = plt.subplots(1, 3, figsize=(20, 6))
                    
                    methods = ['K-Means', 'DBSCAN', 'CAH']
                    labels_list = [kmeans_labels, dbscan_labels, cah_labels]
                    
                    for idx, (method, labels) in enumerate(zip(methods, labels_list)):
                        unique_labels = sorted(set(labels))
                        colors = plt.cm.Set3(np.linspace(0, 1, len(unique_labels)))
                        
                        for i, label in enumerate(unique_labels):
                            mask = labels == label
                            if label == -1:
                                axes[idx].scatter(df_pca.iloc[mask, 0], df_pca.iloc[mask, 1], 
                                                 c='black', label='Bruit', alpha=0.7, marker='x')
                            else:
                                axes[idx].scatter(df_pca.iloc[mask, 0], df_pca.iloc[mask, 1], 
                                                 c=[colors[i]], label=f'Cluster {label}', alpha=0.7)
                        
                        axes[idx].set_xlabel('PC1')
                        axes[idx].set_ylabel('PC2')
                        axes[idx].set_title(f'{method}')
                        axes[idx].legend()
                        axes[idx].grid(True, alpha=0.3)
                    
                    plt.tight_layout()
                    st.pyplot(fig)
                    
                    # Recommandations
                    st.subheader("üí° Recommandations")
                    
                    best_method = max(results.items(), key=lambda x: x[1]['silhouette_score'])
                    st.success(f"üèÜ **Meilleure m√©thode:** {best_method[0]} (Score Silhouette: {best_method[1]['silhouette_score']:.4f})")
                    
                    # Recommandations contextuelles
                    if results['DBSCAN']['noise_points'] > len(df_scaled) * 0.3:
                        st.warning("‚ö†Ô∏è DBSCAN d√©tecte beaucoup de bruit (>30%). Consid√©rez K-Means ou CAH.")
                    
                    if results['K-Means']['silhouette_score'] > 0.5:
                        st.info("üëç K-Means montre une bonne s√©paration des clusters.")
                    
                    if results['CAH']['silhouette_score'] > results['K-Means']['silhouette_score']:
                        st.info("üå≥ CAH pourrait √™tre plus appropri√© pour vos donn√©es hi√©rarchiques.")
            
            except Exception as e:
                st.error(f"‚ùå Une erreur s'est produite lors de l'analyse: {e}")
                st.info("üí° V√©rifiez que vos donn√©es sont bien format√©es et qu'il y a suffisamment d'observations.")

# ============================================
# FONCTIONS AUXILIAIRES POUR LES VISUALISATIONS
# ============================================

def analyze_kmeans_clusters(df_original, df_scaled, cluster_labels, feature_names):
    """Analyse d√©taill√©e des clusters K-Means"""
    st.subheader("üîç Analyse D√©taill√©e des Clusters K-Means")
    
    # Cr√©er un DataFrame avec les r√©sultats
    # Ensure indices match between df_original and df_scaled
    analysis_df = df_original.reset_index(drop=True).iloc[:len(cluster_labels)].copy()
    analysis_df['Cluster'] = cluster_labels
    
    for cluster_id in sorted(set(cluster_labels)):
        with st.expander(f"üìä Cluster {cluster_id} - Analyse", expanded=False):
            cluster_data = analysis_df[analysis_df['Cluster'] == cluster_id]
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.metric("Nombre de propri√©t√©s", len(cluster_data))
                
                if 'price' in cluster_data.columns:
                    st.metric("Prix moyen", f"{cluster_data['price'].mean():.0f} TND")
                    st.metric("Prix m√©dian", f"{cluster_data['price'].median():.0f} TND")
                
                if 'size' in cluster_data.columns:
                    st.metric("Taille moyenne", f"{cluster_data['size'].mean():.0f} m¬≤")
            
            with col2:
                if 'neighborhood' in cluster_data.columns:
                    neighborhoods = cluster_data['neighborhood'].value_counts().head(3)
                    st.write("**Top 3 quartiers:**")
                    for neighborhood, count in neighborhoods.items():
                        st.write(f"‚Ä¢ {neighborhood}: {count} propri√©t√©s")
                
                if 'condition' in cluster_data.columns:
                    conditions = cluster_data['condition'].value_counts().head(3)
                    st.write("**√âtats principaux:**")
                    for condition, count in conditions.items():
                        st.write(f"‚Ä¢ {condition}: {count} propri√©t√©s")

def visualize_dbscan_results(df_pca, cluster_labels, explained_variance):
    """Visualisation sp√©cifique pour DBSCAN"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # 1. Clusters DBSCAN en 2D PCA
    unique_labels = sorted(set(cluster_labels))
    colors = plt.cm.Set3(np.linspace(0, 1, len(unique_labels)))
    
    for i, label in enumerate(unique_labels):
        mask = cluster_labels == label
        if label == -1:
            axes[0,0].scatter(df_pca.iloc[mask, 0], df_pca.iloc[mask, 1], 
                             c='black', label='Bruit', alpha=0.7, marker='x')
        else:
            axes[0,0].scatter(df_pca.iloc[mask, 0], df_pca.iloc[mask, 1], 
                             c=[colors[i]], label=f'Cluster {label}', alpha=0.7)
    
    axes[0,0].set_xlabel('PC1')
    axes[0,0].set_ylabel('PC2')
    axes[0,0].set_title('DBSCAN - Clusters (2D PCA)')
    axes[0,0].legend()
    axes[0,0].grid(True, alpha=0.3)
    
    # 2. Distribution des clusters DBSCAN
    cluster_counts = pd.Series(cluster_labels).value_counts().sort_index()
    cluster_names = ['Bruit' if idx == -1 else f'Cluster {idx}' for idx in cluster_counts.index]
    bar_colors = ['black' if idx == -1 else colors[i] for i, idx in enumerate(cluster_counts.index)]
    
    bars = axes[0,1].bar(range(len(cluster_counts)), cluster_counts.values, color=bar_colors)
    axes[0,1].set_xticks(range(len(cluster_counts)))
    axes[0,1].set_xticklabels(cluster_names, rotation=45)
    axes[0,1].set_ylabel('Nombre de propri√©t√©s')
    axes[0,1].set_title('Distribution des clusters DBSCAN')
    axes[0,1].grid(True, alpha=0.3)
    
    # Ajouter les valeurs sur les barres
    for bar, count in zip(bars, cluster_counts.values):
        axes[0,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, 
                       str(count), ha='center', va='bottom')
    
    # 3. Variance expliqu√©e par PCA
    pc_names = [f'PC{i+1}' for i in range(len(explained_variance))]
    axes[1,0].bar(pc_names, explained_variance * 100, color='lightblue', edgecolor='black')
    axes[1,0].set_xlabel('Composantes principales')
    axes[1,0].set_ylabel('Variance expliqu√©e (%)')
    axes[1,0].set_title('Variance expliqu√©e par PCA')
    axes[1,0].grid(True, alpha=0.3)
    
    # Ajouter les pourcentages sur les barres
    for i, var in enumerate(explained_variance):
        axes[1,0].text(i, var * 100 + 1, f'{var*100:.1f}%', ha='center', va='bottom')
    
    # 4. Histogramme des distances aux points centraux (si applicable)
    axes[1,1].hist(cluster_labels, bins=len(unique_labels), color='lightcoral', edgecolor='black', alpha=0.7)
    axes[1,1].set_xlabel('Cluster ID')
    axes[1,1].set_ylabel('Fr√©quence')
    axes[1,1].set_title('Distribution des assignations de clusters')
    axes[1,1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    st.pyplot(fig)

def analyze_dbscan_clusters(df_original, df_scaled, cluster_labels, feature_names, metrics):
    """Analyse d√©taill√©e des clusters DBSCAN"""
    st.subheader("üîç Analyse D√©taill√©e des Clusters DBSCAN")
    
    # √âvaluation de la qualit√©
    noise_ratio = metrics['noise_ratio']
    n_clusters = metrics['n_clusters']
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if noise_ratio < 0.1:
            st.success("‚úÖ Excellent ratio de bruit (<10%)")
        elif noise_ratio < 0.2:
            st.info("üëç Bon ratio de bruit (<20%)")
        elif noise_ratio < 0.4:
            st.warning("‚ö†Ô∏è Ratio de bruit acceptable (<40%)")
        else:
            st.error("‚ùå Ratio de bruit probl√©matique (>40%)")
    
    with col2:
        if n_clusters >= 2 and n_clusters <= 6:
            st.success(f"‚úÖ Nombre optimal de clusters: {n_clusters}")
        elif n_clusters == 1:
            st.warning("‚ö†Ô∏è Un seul cluster d√©tect√©")
        elif n_clusters == 0:
            st.error("‚ùå Aucun cluster d√©tect√©")
        else:
            st.info(f"üìä {n_clusters} clusters d√©tect√©s")
    
    with col3:
        if metrics['silhouette_score'] > 0.5:
            st.success(f"‚úÖ Excellente coh√©sion: {metrics['silhouette_score']:.3f}")
        elif metrics['silhouette_score'] > 0.3:
            st.info(f"üëç Bonne coh√©sion: {metrics['silhouette_score']:.3f}")
        elif metrics['silhouette_score'] > 0:
            st.warning(f"‚ö†Ô∏è Coh√©sion faible: {metrics['silhouette_score']:.3f}")
        else:
            st.error("‚ùå Impossible de calculer la coh√©sion")
    
    # Analyser chaque cluster + bruit
    analysis_df = df_original.reset_index(drop=True).iloc[:len(cluster_labels)].copy()
    analysis_df['Cluster'] = cluster_labels
    
    unique_labels = sorted(set(cluster_labels))
    
    for label in unique_labels:
        cluster_data = analysis_df[analysis_df['Cluster'] == label]
        
        if label == -1:
            with st.expander(f"üîç Points de Bruit - {len(cluster_data)} propri√©t√©s", expanded=False):
                st.warning("‚ö†Ô∏è Ces propri√©t√©s ont des caract√©ristiques uniques/atypiques")
                
                if 'price' in cluster_data.columns:
                    st.write(f"**Prix:** {cluster_data['price'].min():.0f} - {cluster_data['price'].max():.0f} TND")
                    prix_median_general = analysis_df[analysis_df['Cluster'] != -1]['price'].median()
                    bonnes_affaires = cluster_data[cluster_data['price'] < prix_median_general]
                    if len(bonnes_affaires) > 0:
                        st.info(f"üí∞ {len(bonnes_affaires)} propri√©t√©s sous le prix m√©dian (bonnes affaires potentielles)")
                
                if len(cluster_data) <= 10:
                    st.dataframe(cluster_data[['price', 'size', 'neighborhood'] if all(col in cluster_data.columns for col in ['price', 'size', 'neighborhood']) else cluster_data.columns[:5]])
        else:
            with st.expander(f"üìä Cluster {label} - {len(cluster_data)} propri√©t√©s", expanded=False):
                col1, col2 = st.columns(2)
                
                with col1:
                    if 'price' in cluster_data.columns:
                        st.metric("Prix moyen", f"{cluster_data['price'].mean():.0f} TND")
                        st.metric("√âcart-type prix", f"{cluster_data['price'].std():.0f} TND")
                    
                    if 'size' in cluster_data.columns:
                        st.metric("Taille moyenne", f"{cluster_data['size'].mean():.0f} m¬≤")
                
                with col2:
                    if 'neighborhood' in cluster_data.columns:
                        neighborhoods = cluster_data['neighborhood'].value_counts().head(3)
                        st.write("**Quartiers principaux:**")
                        for neighborhood, count in neighborhoods.items():
                            pct = (count / len(cluster_data)) * 100
                            st.write(f"‚Ä¢ {neighborhood}: {count} ({pct:.0f}%)")
                    
                    if 'condition' in cluster_data.columns:
                        conditions = cluster_data['condition'].value_counts().head(2)
                        st.write("**√âtats principaux:**")
                        for condition, count in conditions.items():
                            pct = (count / len(cluster_data)) * 100
                            st.write(f"‚Ä¢ {condition}: {count} ({pct:.0f}%)")

def create_cluster_profile_radar(df_with_clusters, selected_features, cluster_labels):
    """Cr√©er un graphique radar pour profiler les clusters"""
    unique_clusters = [c for c in sorted(set(cluster_labels)) if c != -1]  # Exclure le bruit
    
    if len(unique_clusters) <= 1 or len(selected_features) <= 2:
        return None
    
    # Calculer les moyennes par cluster
    cluster_means = df_with_clusters.groupby('Cluster')[selected_features].mean()
    
    # Normaliser les valeurs (0-1) pour le radar chart
    from sklearn.preprocessing import MinMaxScaler
    scaler = MinMaxScaler()
    cluster_means_normalized = pd.DataFrame(
        scaler.fit_transform(cluster_means),
        columns=cluster_means.columns,
        index=cluster_means.index
    )
    
    # Cr√©er le graphique radar avec Plotly
    fig = go.Figure()
    
    colors = px.colors.qualitative.Set3
    
    for i, (cluster_id, row) in enumerate(cluster_means_normalized.iterrows()):
        if cluster_id != -1:  # Exclure le bruit
            fig.add_trace(go.Scatterpolar(
                r=row.values.tolist() + [row.values[0]],  # Fermer le polygone
                theta=row.index.tolist() + [row.index[0]],
                fill='toself',
                name=f'Cluster {cluster_id}',
                marker_color=colors[i % len(colors)],
                opacity=0.6
            ))
    
    fig.update_layout(
        polar=dict(
            radialaxis=dict(
                visible=True,
                range=[0, 1],
                tickmode='linear',
                tick0=0,
                dtick=0.2
            )
        ),
        showlegend=True,
        title="Profil des clusters (valeurs normalis√©es 0-1)",
        height=500
    )
    
    return fig

# ============================================
# SECTION D'AIDE ET INTERPR√âTATION
# ============================================

def add_clustering_help_section():
    """Section d'aide pour l'interpr√©tation des r√©sultats"""
    with st.expander("üí° Guide d'Interpr√©tation des R√©sultats de Clustering", expanded=False):
        st.markdown("""
        ## üîµ K-Means
        **Principe :** Divise les donn√©es en k clusters en minimisant la variance intra-cluster.
        
        **M√©triques importantes :**
        - **Score de Silhouette** (0 √† 1) : Mesure la qualit√© du clustering. Plus proche de 1 = meilleur.
        - **Inertie** : Somme des distances au carr√© des points √† leur centro√Øde. Plus faible = mieux.
        - **Score Calinski-Harabasz** : Ratio de dispersion entre/dans les clusters. Plus √©lev√© = mieux.
        
        **Avantages :** Rapide, stable, bon pour clusters sph√©riques.
        **Inconv√©nients :** N√©cessite de sp√©cifier k, sensible aux outliers.
        
        ---
        
        ## üî¥ DBSCAN
        **Principe :** Identifie des clusters de densit√© et marque les points isol√©s comme bruit.
        
        **Param√®tres cl√©s :**
        - **eps** : Distance maximale entre deux points pour qu'ils soient voisins.
        - **min_samples** : Nombre minimum de points pour former un cluster.
        
        **M√©triques importantes :**
        - **Points de bruit** : Points qui ne peuvent √™tre assign√©s √† aucun cluster.
        - **Ratio de bruit** : Pourcentage de points de bruit. <20% = bon, >40% = probl√©matique.
        
        **Avantages :** D√©tecte automatiquement le nombre de clusters, robuste aux outliers, clusters de forme arbitraire.
        **Inconv√©nients :** Sensible aux param√®tres, difficile avec densit√©s variables.
        
        ---
        
        ## üü¢ CAH (Classification Ascendante Hi√©rarchique)
        **Principe :** Construit une hi√©rarchie de clusters en fusionnant progressivement les plus proches.
        
        **M√©thodes de liaison :**
        - **Ward** : Minimise la variance intra-cluster (recommand√©).
        - **Complete** : Distance maximale entre clusters.
        - **Average** : Distance moyenne entre clusters.
        - **Single** : Distance minimale entre clusters.
        
        **Avantages :** Dendrogramme informatif, pas besoin de sp√©cifier k √† l'avance, d√©terministe.
        **Inconv√©nients :** Co√ªteux en calcul, sensible au bruit.
        
        ---
        
        ## üìä Interpr√©tation Business
        
        ### Types de Segments Immobiliers Typiques :
        
        **üè† Segment √âconomique**
        - Prix bas, tailles modestes
        - Cible : √âtudiants, jeunes professionnels
        - Strat√©gie : Accessibilit√©, localisation transport
        
        **üè° Segment Familial**
        - Prix moyen, surfaces g√©n√©reuses
        - Cible : Familles, primo-acc√©dants
        - Strat√©gie : Rapport qualit√©/prix, commodit√©s
        
        **üíé Segment Premium**
        - Prix √©lev√©, √©quipements haut de gamme
        - Cible : Cadres, investisseurs
        - Strat√©gie : Luxe, services, localisations privil√©gi√©es
        
        **üîç Points Atypiques (Bruit DBSCAN)**
        - Propri√©t√©s uniques ou mal valoris√©es
        - Opportunit√©s d'investissement potentielles
        - √Ä investiguer individuellement
        
        ---
        
        ## üéØ Conseils d'Analyse
        
        1. **Validation M√©tier** : Les clusters doivent avoir du sens business.
        2. **Taille des Clusters** : √âviter les clusters trop petits (<5% des donn√©es).
        3. **Stabilit√©** : Tester plusieurs algorithmes pour confirmer.
        4. **Actionabilit√©** : Chaque cluster doit permettre des actions marketing distinctes.
        5. **√âquilibrage** : Pr√©f√©rer des clusters de tailles relativement √©quilibr√©es.
        
        ### Signaux d'Alerte :
        - Score Silhouette < 0.2 : Clustering peu fiable
        - Trop de bruit DBSCAN (>50%) : Revoir les param√®tres
        - Un seul gros cluster : Donn√©es trop homog√®nes ou param√®tres inad√©quats
        """)

# Ajouter la section d'aide √† la fin de la fonction principale
def unsupervised_learning_section_complete(df, filtered_df):
    """Version compl√®te avec section d'aide"""
    # Appeler la fonction principale
    unsupervised_learning_section(df, filtered_df)
    
    # Ajouter la section d'aide
    add_clustering_help_section()

# ============================================
# FONCTIONS UTILITAIRES SUPPL√âMENTAIRES
# ============================================

def export_clustering_results(df_with_clusters, algorithm_name, cluster_labels):
    """Permettre l'export des r√©sultats de clustering"""
    st.subheader(f"üíæ Export des R√©sultats {algorithm_name}")
    
    if st.button(f"T√©l√©charger les r√©sultats {algorithm_name}"):
        # Pr√©parer les donn√©es pour export
        export_df = df_with_clusters.copy()
        export_df[f'Cluster_{algorithm_name}'] = cluster_labels
        
        # Convertir en CSV
        csv = export_df.to_csv(index=False)
        
        st.download_button(
            label=f"üìÅ T√©l√©charger CSV avec clusters {algorithm_name}",
            data=csv,
            file_name=f"clustering_results_{algorithm_name}_{pd.Timestamp.now().strftime('%Y%m%d_%H%M')}.csv",
            mime="text/csv"
        )
        
        st.success("‚úÖ Fichier pr√™t pour t√©l√©chargement!")

def display_cluster_statistics_summary(results_dict):
    """Afficher un r√©sum√© statistique des diff√©rents algorithmes"""
    st.subheader("üìà R√©sum√© Statistique Comparatif")
    
    summary_data = []
    for method, result in results_dict.items():
        summary_data.append({
            'Algorithme': method,
            'Nombre de Clusters': result['n_clusters'],
            'Score Silhouette': f"{result['silhouette_score']:.4f}",
            'Points de Bruit': result.get('noise_points', 0),
            'Recommandation': get_algorithm_recommendation(result)
        })
    
    summary_df = pd.DataFrame(summary_data)
    st.dataframe(summary_df, use_container_width=True)

def get_algorithm_recommendation(result):
    """G√©n√©rer une recommandation bas√©e sur les r√©sultats"""
    silhouette = result['silhouette_score']
    n_clusters = result['n_clusters']
    noise_points = result.get('noise_points', 0)
    
    if silhouette > 0.6:
        return "üåü Excellent"
    elif silhouette > 0.4:
        return "üëç Bon"
    elif silhouette > 0.2:
        return "‚ö†Ô∏è Acceptable"
    elif n_clusters == 0:
        return "‚ùå √âchec"
    else:
        return "üîÑ √Ä revoir"
# Application principale
# Initialiser les variables de session
if 'df_imputed' not in st.session_state:
    st.session_state['df_imputed'] = None

# T√©l√©chargement du fichier

if uploaded_file is not None:
    try:
        # Essayer de charger avec diff√©rents s√©parateurs
        df = None
        try:
            df = pd.read_csv(uploaded_file, sep=',')
        except:
            try:
                uploaded_file.seek(0)
                df = pd.read_csv(uploaded_file, sep=';')
            except:
                try:
                    uploaded_file.seek(0)
                    df = pd.read_csv(uploaded_file, sep='\t')
                except Exception as e:
                    st.error(f"Impossible de lire le fichier CSV: {e}")
        
        if df is None or df.empty:
            st.error("Le fichier est vide ou n'a pas pu √™tre lu correctement.")
        else:
            # Pr√©traiter les donn√©es
            df = preprocess_data(df)
            
            # Utiliser les donn√©es imput√©es si disponibles
            if st.session_state['df_imputed'] is not None:
                df = st.session_state['df_imputed']
                st.success("Utilisation des donn√©es imput√©es!")
            else:
                st.success("Fichier charg√© avec succ√®s!")
            
            # Filtres dans la barre lat√©rale
            st.sidebar.title("Filtres")
            
            if 'transaction' in df.columns:
                unique_transactions = sorted(df['transaction'].dropna().unique().tolist())
                display_transactions = ["Tous"] + [t.capitalize() for t in unique_transactions if isinstance(t, str)]
                transaction_map = {t.capitalize(): t for t in unique_transactions if isinstance(t, str)}
                transaction_map["Tous"] = "Tous"
                
                selected_display_transaction = st.sidebar.selectbox("Type de Transaction", display_transactions)
                selected_transaction = transaction_map[selected_display_transaction]
            else:
                selected_transaction = "Tous"
            
            if 'property_type' in df.columns:
                unique_property_types = sorted(df['property_type'].dropna().unique().tolist())
                display_property_types = ["Tous"] + [p.capitalize() for p in unique_property_types if isinstance(p, str)]
                property_type_map = {p.capitalize(): p for p in unique_property_types if isinstance(p, str)}
                property_type_map["Tous"] = "Tous"
                
                selected_display_property = st.sidebar.selectbox("Type de Bien", display_property_types)
                selected_property = property_type_map[selected_display_property]
            else:
                selected_property = "Tous"
            
            if 'city' in df.columns:
                unique_cities = sorted(df['city'].dropna().unique().tolist())
                display_cities = ["Toutes"] + [c.title() for c in unique_cities if isinstance(c, str)]
                city_map = {c.title(): c for c in unique_cities if isinstance(c, str)}
                city_map["Toutes"] = "Toutes"
                
                selected_display_city = st.sidebar.selectbox("Ville", display_cities)
                selected_city = city_map[selected_display_city]
            else:
                selected_city = "Toutes"
            
            # Appliquer les filtres
            filtered_df = df.copy()
            if selected_transaction != "Tous" and 'transaction' in df.columns:
                filtered_df = filtered_df[filtered_df['transaction'] == selected_transaction]
            if selected_property != "Tous" and 'property_type' in df.columns:
                filtered_df = filtered_df[filtered_df['property_type'] == selected_property]
            if selected_city != "Toutes" and 'city' in df.columns:
                filtered_df = filtered_df[filtered_df['city'] == selected_city]
            
            # Cr√©er des onglets pour organiser l'interface
            tab1, tab2, tab3, tab4, tab5,tab6 = st.tabs(["Aper√ßu des donn√©es", "Statistiques de base","Visualisations avanc√©es", "Imputation" , "Apprentissage supervis√©","Apprentissage non supervis√©"])
            
            with tab1:
                st.subheader("Aper√ßu des donn√©es")
                st.dataframe(filtered_df.head())
                
                st.subheader("Informations sur les colonnes")
                col1, col2 = st.columns(2)
                
                with col1:
                    st.write("Types de donn√©es:")
                    st.write(pd.DataFrame({'Type': filtered_df.dtypes}))
                
                with col2:
                    st.write("Valeurs manquantes:")
                    missing_data = pd.DataFrame({
                        'Manquantes': filtered_df.isna().sum(), 
                        'Pourcentage': (filtered_df.isna().sum() / len(filtered_df) * 100).round(2)
                    })
                    st.write(missing_data)
            
            with tab2:
                # Statistiques de base
                st.subheader("Statistiques descriptives")
                
                # S√©lectionner uniquement les colonnes num√©riques pour describe()
                numeric_df = filtered_df.select_dtypes(include=['number'])
                if not numeric_df.empty:
                    st.dataframe(numeric_df.describe())
                else:
                    st.warning("Aucune colonne num√©rique disponible pour l'analyse statistique.")
                
                # M√©triques cl√©s
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    st.metric("Nombre de propri√©t√©s", len(filtered_df))
                
                with col2:
                    if 'price' in filtered_df.columns:
                        avg_price_sale = filtered_df.loc[filtered_df['transaction'] == 'sale', 'price'].dropna().mean()
                        if pd.notna(avg_price_sale):
                            st.metric("Prix moyen de vente", f"{avg_price_sale:,.0f} TND")
                        else:
                            st.metric("Prix moyen", "N/A")
                        avg_price_rent = filtered_df.loc[filtered_df['transaction'] == 'rent', 'price'].dropna().mean()
                        if pd.notna(avg_price_rent):
                            st.metric("Prix moyen de location", f"{avg_price_rent:,.0f} TND")
                    else:
                        st.metric("Prix moyen", "Colonne manquante")
                
                with col3:
                    if 'size' in filtered_df.columns:
                        avg_size = filtered_df['size'].dropna().mean()
                        if pd.notna(avg_size):
                            st.metric("Surface moyenne", f"{avg_size:.1f} m¬≤")
                        else:
                            st.metric("Surface moyenne", "N/A")
                    else:
                        st.metric("Surface moyenne", "Colonne manquante")
                
                # Visualisations de base
                basic_visualizations(filtered_df)
            with tab3:
                # Visualisations avanc√©es
                advanced_visualizations(filtered_df)
                
            with tab4:
                # Section d'imputation
                updated_df = imputation_section(df)
                if updated_df is not None:
                    df = updated_df  # Mettre √† jour le dataframe avec les donn√©es imput√©es
            
            with tab5:
                # Section d'apprentissage supervis√©
                supervised_learning_section(df, filtered_df)
                
            with tab6:
                unsupervised_learning_section(df, filtered_df)
        
    except Exception as e:
        st.error(f"Une erreur s'est produite lors du traitement du fichier: {e}")
        st.info("Conseil: V√©rifiez le format de votre fichier CSV et assurez-vous que les colonnes sont correctement nomm√©es.")