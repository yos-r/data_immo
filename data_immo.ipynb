{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ce5bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports nécessaires\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import missingno as msno\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('ggplot')\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set(style=\"whitegrid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7878ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données\n",
    "df = pd.read_csv('data.csv')\n",
    "print(f\"Aperçu des données ({df.shape[0]} lignes, {df.shape[1]} colonnes):\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2851ee95",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = ['listing_price','price_ttc','price', 'size', 'rooms', 'bedrooms', 'bathrooms', 'parkings', \n",
    "                  'construction_year', 'age', 'air_conditioning', 'central_heating', \n",
    "                  'swimming_pool', 'elevator', 'garden', 'equipped_kitchen']\n",
    "for col in numeric_columns:\n",
    "    if col in df.columns:\n",
    "        # Afficher le type original\n",
    "        original_type = df[col].dtype\n",
    "        \n",
    "        # Convertir en numérique\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        \n",
    "        # Afficher les informations sur la conversion\n",
    "        na_count = df[col].isna().sum()\n",
    "        print(f\"Conversion de '{col}': {original_type} -> {df[col].dtype}, valeurs NA créées: {na_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8499e717",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nInformations sur les types de données après conversion:\")\n",
    "display(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47a1377",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nStatistiques descriptives:\")\n",
    "display(df.describe().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2a1a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse détaillée des valeurs manquantes\n",
    "def analyze_missing_data(df):\n",
    "    # Calculer les informations sur les valeurs manquantes\n",
    "    missing_count = df.isna().sum()\n",
    "    missing_percent = (df.isna().sum() / len(df) * 100).round(2)\n",
    "    \n",
    "    # Créer un DataFrame avec ces informations\n",
    "    missing_data = pd.DataFrame({\n",
    "        'Type de données': df.dtypes,\n",
    "        'Valeurs non-NA': df.count(),\n",
    "        'Valeurs NA': missing_count,\n",
    "        'Pourcentage NA (%)': missing_percent,\n",
    "        'Valeurs uniques': df.nunique()\n",
    "    })\n",
    "    \n",
    "    # Trier par pourcentage de valeurs manquantes (décroissant)\n",
    "    missing_data = missing_data.sort_values('Pourcentage NA (%)', ascending=False)\n",
    "    \n",
    "    return missing_data\n",
    "\n",
    "# Générer le récapitulatif des données manquantes\n",
    "missing_summary = analyze_missing_data(df)\n",
    "\n",
    "# Afficher le récapitulatif complet\n",
    "print(\"\\nRécapitulatif détaillé des données manquantes:\")\n",
    "display(missing_summary)\n",
    "\n",
    "# Visualiser les données manquantes sous forme de graphique\n",
    "plt.figure(figsize=(14, 10))\n",
    "ax = sns.barplot(x=missing_summary.index, \n",
    "                 y='Pourcentage NA (%)', \n",
    "                 data=missing_summary,\n",
    "                 palette='YlOrRd')\n",
    "plt.title('Pourcentage de valeurs manquantes par colonne')\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylim(0, 100)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Ajouter les pourcentages sur les barres\n",
    "for i, value in enumerate(missing_summary['Pourcentage NA (%)']):\n",
    "    if value > 0:  # Afficher uniquement les valeurs non nulles\n",
    "        ax.text(i, value + 1, f'{value}%', ha='center', va='bottom')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Identifier les colonnes avec beaucoup de valeurs manquantes\n",
    "high_missing_cols = missing_summary[missing_summary['Pourcentage NA (%)'] > 30]\n",
    "if not high_missing_cols.empty:\n",
    "    print(\"\\nColonnes avec plus de 30% de valeurs manquantes:\")\n",
    "    display(high_missing_cols)\n",
    "\n",
    "# Identifier les colonnes sans valeurs manquantes\n",
    "no_missing_cols = missing_summary[missing_summary['Pourcentage NA (%)'] == 0]\n",
    "if not no_missing_cols.empty:\n",
    "    print(\"\\nColonnes sans valeurs manquantes:\")\n",
    "    display(no_missing_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaf75d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valeurs uniques pour les colonnes catégorielles\n",
    "unique_transactions = df['transaction'].unique()\n",
    "unique_sources = df['source'].unique()\n",
    "unique_property_types = df['property_type'].unique()\n",
    "unique_finishings = df['finishing'].unique()\n",
    "unique_conditions = df['condition'].unique()\n",
    "unique_neighborhoods= df['neighborhood'].unique()\n",
    "print(\"\\nValeurs uniques pour la colonne 'transaction':\")\n",
    "print(unique_transactions)\n",
    "print(\"\\nValeurs uniques pour la colonne 'finishing':\")\n",
    "print(unique_finishings)\n",
    "print(\"Valeurs uniques pour la colonne 'condition':\")\n",
    "print(unique_conditions)\n",
    "print(\"\\nValeurs uniques pour la colonne 'property_type':\")\n",
    "print(unique_property_types)\n",
    "print(\"Valeurs uniques pour la colonne 'source':\")\n",
    "print(unique_sources)\n",
    "\n",
    "print(\"\\nValeurs uniques pour la colonne 'neighborhood':\")\n",
    "print(unique_neighborhoods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b91e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation des valeurs manquantes\n",
    "# Données numériques : \n",
    "\n",
    "#imputation des prix manquants par la moyenne  pour chaque quartier, type de propriété et transaction \n",
    "# exemple : Centre Urbain Nord, Appartement, Vente\n",
    "df['price'] = df.groupby(['neighborhood', 'property_type','transaction'])['price'].transform(\n",
    "    lambda x: x.fillna(x.mean())\n",
    ")\n",
    "df['price_ttc'] = df.groupby(['neighborhood', 'property_type','transaction'])['price_ttc'].transform(\n",
    "    lambda x: x.fillna(x.mean())\n",
    ")\n",
    "# Remplir les valeurs manquantes de 'listing_price' avec la valeur de 'price' si disponible\n",
    "df['listing_price'] = df['listing_price'].fillna(df['price'])\n",
    "# remplacer suffixe par ttc par defaut\n",
    "df['suffix'] = df['suffix'].fillna('TTC')\n",
    "\n",
    "\n",
    "# Afficher les lignes où 'price' est toujours manquant après l'imputation\n",
    "null_price_rows = df[df['price'].isna()]\n",
    "display(null_price_rows)\n",
    "print(f\"Nombre de lignes avec 'price' manquant après imputation : {null_price_rows.shape[0]}\") # de 244 prix manquants on passe à 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034359af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables catégorielles: imputation par la valeur la plus fréquente\n",
    "# Condition \n",
    "\n",
    "def impute_condition_simple(df):\n",
    "    \"\"\"\n",
    "    Impute les valeurs manquantes dans la colonne 'condition' en se basant sur:\n",
    "    1. La zone (neighborhood ou city)\n",
    "    2. Le type de transaction\n",
    "    3. L'intervalle de prix\n",
    "    \n",
    "    Cette version suppose qu'il n'y a pas de valeurs manquantes dans \n",
    "    les colonnes de groupement (price, neighborhood/city, transaction).\n",
    "    \"\"\"\n",
    "    # Créer une copie pour ne pas modifier le dataframe original\n",
    "    df_imputed = df.copy()\n",
    "    \n",
    "    # Vérifier s'il y a des valeurs manquantes dans la colonne condition\n",
    "    missing_count = df_imputed['condition'].isna().sum()\n",
    "    if missing_count == 0:\n",
    "        print(\"Aucune valeur manquante dans la colonne 'condition'. Aucune imputation nécessaire.\")\n",
    "        return df_imputed\n",
    "    \n",
    "    print(f\"Imputation de {missing_count} valeurs manquantes dans la colonne 'condition'...\")\n",
    "    \n",
    "    # 1. Déterminer quelle colonne de zone utiliser\n",
    "    zone_column = 'neighborhood' if 'neighborhood' in df_imputed.columns else 'city'\n",
    "    print(f\"Utilisation de '{zone_column}' comme colonne de zone géographique\")\n",
    "    \n",
    "    # 2. Créer des intervalles de prix pour le groupement\n",
    "    # Calculer les quantiles pour créer des segments de prix équilibrés\n",
    "    price_bins = [0] + list(df_imputed['price'].quantile([0.25, 0.5, 0.75, 1.0]))\n",
    "    price_labels = ['Bas', 'Moyen-bas', 'Moyen-haut', 'Élevé']\n",
    "    \n",
    "    # Créer une colonne pour l'intervalle de prix\n",
    "    df_imputed['price_range'] = pd.cut(df_imputed['price'], \n",
    "                                     bins=price_bins, \n",
    "                                     labels=price_labels,\n",
    "                                     include_lowest=True)\n",
    "    \n",
    "    # 3. Approche par niveaux pour l'imputation\n",
    "    # Masque initial pour les lignes avec condition manquante\n",
    "    missing_mask = df_imputed['condition'].isna()\n",
    "    \n",
    "    # Niveau 1: Imputation basée sur zone + transaction + intervalle de prix\n",
    "    for index, row in df_imputed[missing_mask].iterrows():\n",
    "        # Trouver des propriétés similaires avec la même zone, transaction et intervalle de prix\n",
    "        similar_props = df_imputed[\n",
    "            (df_imputed[zone_column] == row[zone_column]) & \n",
    "            (df_imputed['transaction'] == row['transaction']) & \n",
    "            (df_imputed['price_range'] == row['price_range']) & \n",
    "            (~df_imputed['condition'].isna())\n",
    "        ]\n",
    "        \n",
    "        # S'il y a des propriétés similaires, utiliser leur condition la plus fréquente\n",
    "        if len(similar_props) > 0:\n",
    "            df_imputed.loc[index, 'condition'] = similar_props['condition'].mode()[0]\n",
    "    \n",
    "    # Mise à jour du masque après le premier niveau d'imputation\n",
    "    missing_mask = df_imputed['condition'].isna()\n",
    "    remaining = missing_mask.sum()\n",
    "    \n",
    "    if remaining > 0:\n",
    "        print(f\"Niveau 1 terminé. {remaining} valeurs restent à imputer.\")\n",
    "        \n",
    "        # Niveau 2: Imputation basée sur transaction + intervalle de prix\n",
    "        for index, row in df_imputed[missing_mask].iterrows():\n",
    "            similar_props = df_imputed[\n",
    "                (df_imputed['transaction'] == row['transaction']) & \n",
    "                (df_imputed['price_range'] == row['price_range']) & \n",
    "                (~df_imputed['condition'].isna())\n",
    "            ]\n",
    "            \n",
    "            if len(similar_props) > 0:\n",
    "                df_imputed.loc[index, 'condition'] = similar_props['condition'].mode()[0]\n",
    "        \n",
    "        # Mise à jour du masque après le deuxième niveau\n",
    "        missing_mask = df_imputed['condition'].isna()\n",
    "        remaining = missing_mask.sum()\n",
    "        \n",
    "        if remaining > 0:\n",
    "            print(f\"Niveau 2 terminé. {remaining} valeurs restent à imputer.\")\n",
    "            \n",
    "            # Niveau 3: Imputation basée sur l'intervalle de prix uniquement\n",
    "            for index, row in df_imputed[missing_mask].iterrows():\n",
    "                similar_props = df_imputed[\n",
    "                    (df_imputed['price_range'] == row['price_range']) & \n",
    "                    (~df_imputed['condition'].isna())\n",
    "                ]\n",
    "                \n",
    "                if len(similar_props) > 0:\n",
    "                    df_imputed.loc[index, 'condition'] = similar_props['condition'].mode()[0]\n",
    "            \n",
    "            # Mise à jour du masque après le troisième niveau\n",
    "            missing_mask = df_imputed['condition'].isna()\n",
    "            remaining = missing_mask.sum()\n",
    "            \n",
    "            if remaining > 0:\n",
    "                print(f\"Niveau 3 terminé. {remaining} valeurs restent à imputer.\")\n",
    "                \n",
    "                # Niveau 4: Imputation globale avec la valeur la plus fréquente\n",
    "                most_common = df_imputed['condition'].dropna().mode()[0]\n",
    "                df_imputed.loc[missing_mask, 'condition'] = most_common\n",
    "                print(f\"Niveau 4 terminé. Imputation globale effectuée.\")\n",
    "    \n",
    "    # Supprimer la colonne temporaire d'intervalle de prix\n",
    "    df_imputed.drop('price_range', axis=1, inplace=True)\n",
    "    \n",
    "    # Vérification finale\n",
    "    final_missing = df_imputed['condition'].isna().sum()\n",
    "    if final_missing == 0:\n",
    "        print(\"Imputation réussie ! Toutes les valeurs manquantes de 'condition' ont été imputées.\")\n",
    "    else:\n",
    "        print(f\"Attention : {final_missing} valeurs restent manquantes après imputation.\")\n",
    "    \n",
    "    return df_imputed\n",
    "\n",
    "# Utilisation de la fonction\n",
    "df_with_imputed_condition = impute_condition_simple(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1d8284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables catégorielles: imputation par la valeur la plus fréquente par zone/ intervalle de prix et transaction\n",
    "# Standing \n",
    "\n",
    "def impute_finishing_simple(df):\n",
    "    \"\"\"\n",
    "    Impute les valeurs manquantes dans la colonne 'finishing' en se basant sur:\n",
    "    1. La zone (neighborhood ou city)\n",
    "    2. Le type de transaction\n",
    "    3. L'intervalle de prix\n",
    "    \n",
    "    Cette version suppose qu'il n'y a pas de valeurs manquantes dans \n",
    "    les colonnes de groupement (price, neighborhood/city, transaction).\n",
    "    \"\"\"\n",
    "    # Créer une copie pour ne pas modifier le dataframe original\n",
    "    df_imputed = df.copy()\n",
    "    \n",
    "    # Vérifier s'il y a des valeurs manquantes dans la colonne condition\n",
    "    missing_count = df_imputed['finishing'].isna().sum()\n",
    "    if missing_count == 0:\n",
    "        print(\"Aucune valeur manquante dans la colonne 'finishing'. Aucune imputation nécessaire.\")\n",
    "        return df_imputed\n",
    "    \n",
    "    print(f\"Imputation de {missing_count} valeurs manquantes dans la colonne 'finishing'...\")\n",
    "    \n",
    "    # 1. Déterminer quelle colonne de zone utiliser\n",
    "    zone_column = 'neighborhood' if 'neighborhood' in df_imputed.columns else 'city'\n",
    "    print(f\"Utilisation de '{zone_column}' comme colonne de zone géographique\")\n",
    "    \n",
    "    # 2. Créer des intervalles de prix pour le groupement\n",
    "    # Calculer les quantiles pour créer des segments de prix équilibrés\n",
    "    price_bins = [0] + list(df_imputed['price'].quantile([0.25, 0.5, 0.75, 1.0]))\n",
    "    price_labels = ['Bas', 'Moyen-bas', 'Moyen-haut', 'Élevé']\n",
    "    \n",
    "    # Créer une colonne pour l'intervalle de prix\n",
    "    df_imputed['price_range'] = pd.cut(df_imputed['price'], \n",
    "                                     bins=price_bins, \n",
    "                                     labels=price_labels,\n",
    "                                     include_lowest=True)\n",
    "    \n",
    "    # 3. Approche par niveaux pour l'imputation\n",
    "    # Masque initial pour les lignes avec standing manquant\n",
    "    missing_mask = df_imputed['finishing'].isna()\n",
    "    \n",
    "    # Niveau 1: Imputation basée sur zone + transaction + intervalle de prix\n",
    "    for index, row in df_imputed[missing_mask].iterrows():\n",
    "        # Trouver des propriétés similaires avec la même zone, transaction et intervalle de prix\n",
    "        similar_props = df_imputed[\n",
    "            (df_imputed[zone_column] == row[zone_column]) & \n",
    "            (df_imputed['transaction'] == row['transaction']) & \n",
    "            (df_imputed['price_range'] == row['price_range']) & \n",
    "            (~df_imputed['finishing'].isna())\n",
    "        ]\n",
    "        \n",
    "        # S'il y a des propriétés similaires, utiliser leur condition la plus fréquente\n",
    "        if len(similar_props) > 0:\n",
    "            df_imputed.loc[index, 'finishing'] = similar_props['finishing'].mode()[0]\n",
    "    \n",
    "    # Mise à jour du masque après le premier niveau d'imputation\n",
    "    missing_mask = df_imputed['finishing'].isna()\n",
    "    remaining = missing_mask.sum()\n",
    "    \n",
    "    if remaining > 0:\n",
    "        print(f\"Niveau 1 terminé. {remaining} valeurs restent à imputer.\")\n",
    "        \n",
    "        # Niveau 2: Imputation basée sur transaction + intervalle de prix\n",
    "        for index, row in df_imputed[missing_mask].iterrows():\n",
    "            similar_props = df_imputed[\n",
    "                (df_imputed['transaction'] == row['transaction']) & \n",
    "                (df_imputed['price_range'] == row['price_range']) & \n",
    "                (~df_imputed['finishing'].isna())\n",
    "            ]\n",
    "            \n",
    "            if len(similar_props) > 0:\n",
    "                df_imputed.loc[index, 'finishing'] = similar_props['finishing'].mode()[0]\n",
    "        \n",
    "        # Mise à jour du masque après le deuxième niveau\n",
    "        missing_mask = df_imputed['finishing'].isna()\n",
    "        remaining = missing_mask.sum()\n",
    "        \n",
    "        if remaining > 0:\n",
    "            print(f\"Niveau 2 terminé. {remaining} valeurs restent à imputer.\")\n",
    "            \n",
    "            # Niveau 3: Imputation basée sur l'intervalle de prix uniquement\n",
    "            for index, row in df_imputed[missing_mask].iterrows():\n",
    "                similar_props = df_imputed[\n",
    "                    (df_imputed['price_range'] == row['price_range']) & \n",
    "                    (~df_imputed['finishing'].isna())\n",
    "                ]\n",
    "                \n",
    "                if len(similar_props) > 0:\n",
    "                    df_imputed.loc[index, 'finishing'] = similar_props['condition'].mode()[0]\n",
    "            \n",
    "            # Mise à jour du masque après le troisième niveau\n",
    "            missing_mask = df_imputed['finishing'].isna()\n",
    "            remaining = missing_mask.sum()\n",
    "            \n",
    "            if remaining > 0:\n",
    "                print(f\"Niveau 3 terminé. {remaining} valeurs restent à imputer.\")\n",
    "                \n",
    "                # Niveau 4: Imputation globale avec la valeur la plus fréquente\n",
    "                most_common = df_imputed['finishing'].dropna().mode()[0]\n",
    "                df_imputed.loc[missing_mask, 'finishing'] = most_common\n",
    "                print(f\"Niveau 4 terminé. Imputation globale effectuée.\")\n",
    "    \n",
    "    # Supprimer la colonne temporaire d'intervalle de prix\n",
    "    df_imputed.drop('price_range', axis=1, inplace=True)\n",
    "    \n",
    "    # Vérification finale\n",
    "    final_missing = df_imputed['finishing'].isna().sum()\n",
    "    if final_missing == 0:\n",
    "        print(\"Imputation réussie ! Toutes les valeurs manquantes de 'finishing' ont été imputées.\")\n",
    "    else:\n",
    "        print(f\"Attention : {final_missing} valeurs restent manquantes après imputation.\")\n",
    "    \n",
    "    return df_imputed\n",
    "\n",
    "# Utilisation de la fonction\n",
    "df_with_imputed_finishing = impute_finishing_simple(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef876b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['condition'] = df_with_imputed_condition['condition']\n",
    "df['finishing'] = df_with_imputed_finishing['finishing']\n",
    "# Vérification des valeurs manquantes restantes\n",
    "missing_summary_after_imputation = analyze_missing_data(df)\n",
    "missing_summary_after_imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cf6212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_property_year_age(df, impute_year=True, impute_age=True, method='grouped_median'):\n",
    "    \"\"\"\n",
    "    Impute les valeurs manquantes dans les colonnes 'year' (année de construction) \n",
    "    et 'age' (âge de la propriété) d'un jeu de données immobilier.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame contenant les données immobilières\n",
    "    impute_year : bool, default=True\n",
    "        Si True, impute les valeurs manquantes dans la colonne 'year'\n",
    "    impute_age : bool, default=True\n",
    "        Si True, impute les valeurs manquantes dans la colonne 'age'\n",
    "    method : str, default='grouped_median'\n",
    "        Méthode d'imputation à utiliser ('grouped_median', 'regression', 'knn')\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame avec les valeurs imputées\n",
    "    \"\"\"\n",
    "    # Créer une copie pour ne pas modifier le dataframe original\n",
    "    df_imputed = df.copy()\n",
    "    \n",
    "    # Vérifier si les colonnes existent\n",
    "    has_year = 'year' in df_imputed.columns\n",
    "    has_age = 'age' in df_imputed.columns\n",
    "    \n",
    "\n",
    "    # 1. Synchronisation année/âge avant imputation\n",
    "    current_year = 2025  \n",
    "    \n",
    "    # Si l'une des colonnes a une valeur mais pas l'autre, compléter l'autre\n",
    "    \n",
    "    \n",
    "    # 2. Imputation des valeurs restantes\n",
    "    \n",
    "    # Méthode 1: Imputation par médiane groupée\n",
    "    if method == 'grouped_median':\n",
    "        # Déterminer les variables de groupement\n",
    "        grouping_cols = []\n",
    "        \n",
    "        # Utiliser le quartier si disponible, sinon la ville\n",
    "        if 'neighborhood' in df_imputed.columns:\n",
    "            grouping_cols.append('neighborhood')\n",
    "        elif 'city' in df_imputed.columns:\n",
    "            grouping_cols.append('city')\n",
    "        \n",
    "        # Ajouter le type de propriété s'il existe\n",
    "        if 'property_type' in df_imputed.columns:\n",
    "            grouping_cols.append('property_type')\n",
    "        \n",
    "        # Pour l'année et l'âge, on peut utiliser l'intervalle de prix\n",
    "        if 'price' in df_imputed.columns:\n",
    "            # Créer des intervalles de prix\n",
    "            price_bins = [0] + list(df_imputed['price'].quantile([0.25, 0.5, 0.75, 1.0]))\n",
    "            price_labels = ['Bas', 'Moyen-bas', 'Moyen-haut', 'Élevé']\n",
    "            \n",
    "            df_imputed['price_range'] = pd.cut(df_imputed['price'], \n",
    "                                            bins=price_bins, \n",
    "                                            labels=price_labels,\n",
    "                                            include_lowest=True)\n",
    "            \n",
    "            grouping_cols.append('price_range')\n",
    "        \n",
    "        print(f\"Variables de groupement utilisées: {', '.join(grouping_cols)}\")\n",
    "        \n",
    "        # Si aucune variable de groupement n'est disponible\n",
    "        if not grouping_cols:\n",
    "            if impute_year and has_year:\n",
    "                median_year = df_imputed['construction_year'].median()\n",
    "                year_missing = df_imputed['construction_year'].isna().sum()\n",
    "                df_imputed['construction_year'].fillna(median_year, inplace=True)\n",
    "                print(f\"Imputé {year_missing} valeurs manquantes dans 'year' avec la médiane globale: {median_year}\")\n",
    "            \n",
    "            if impute_age and has_age:\n",
    "                median_age = df_imputed['age'].median()\n",
    "                age_missing = df_imputed['age'].isna().sum()\n",
    "                df_imputed['age'].fillna(median_age, inplace=True)\n",
    "                print(f\"Imputé {age_missing} valeurs manquantes dans 'age' avec la médiane globale: {median_age}\")\n",
    "            \n",
    "            # Nettoyer et retourner\n",
    "            if 'price_range' in df_imputed.columns and 'price_range' not in df.columns:\n",
    "                df_imputed.drop('price_range', axis=1, inplace=True)\n",
    "            \n",
    "            return df_imputed\n",
    "        \n",
    "        # Imputation par niveaux, du plus spécifique au plus général\n",
    "        \n",
    "        # Générer toutes les combinaisons de variables de groupement\n",
    "        from itertools import combinations\n",
    "        all_combinations = []\n",
    "        \n",
    "        for r in range(len(grouping_cols), 0, -1):\n",
    "            all_combinations.extend(combinations(grouping_cols, r))\n",
    "        \n",
    "        # Imputation pour chaque colonne\n",
    "        columns_to_impute = []\n",
    "        if impute_year and has_year:\n",
    "            columns_to_impute.append('construction_year')\n",
    "        if impute_age and has_age:\n",
    "            columns_to_impute.append('age')\n",
    "        \n",
    "        for col in columns_to_impute:\n",
    "            missing_mask = df_imputed[col].isna()\n",
    "            total_missing = missing_mask.sum()\n",
    "            \n",
    "            if total_missing == 0:\n",
    "                print(f\"Aucune valeur manquante dans '{col}'.\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"Imputation de {total_missing} valeurs manquantes dans '{col}'...\")\n",
    "            \n",
    "            # Parcourir chaque combinaison de groupes\n",
    "            for i, group_vars in enumerate(all_combinations):\n",
    "                if not missing_mask.any():\n",
    "                    break\n",
    "                \n",
    "                print(f\"  Niveau {i+1}: Groupement par {', '.join(group_vars)}\")\n",
    "                \n",
    "                # Pour chaque groupe, calculer la médiane\n",
    "                group_medians = df_imputed[~df_imputed[col].isna()].groupby(list(group_vars))[col].median()\n",
    "                \n",
    "                # Pour chaque ligne avec valeur manquante\n",
    "                for index, row in df_imputed[missing_mask].iterrows():\n",
    "                    # Créer la clé de groupe\n",
    "                    group_key = tuple(row[var] for var in group_vars)\n",
    "                    \n",
    "                    # Si la médiane existe pour ce groupe\n",
    "                    if group_key in group_medians:\n",
    "                        df_imputed.loc[index, col] = group_medians[group_key]\n",
    "                \n",
    "                # Mettre à jour le masque\n",
    "                missing_mask = df_imputed[col].isna()\n",
    "                remaining = missing_mask.sum()\n",
    "                \n",
    "                print(f\"    → {total_missing - remaining}/{total_missing} valeurs imputées ({(total_missing - remaining)/total_missing*100:.1f}%)\")\n",
    "                \n",
    "                if not missing_mask.any():\n",
    "                    print(f\"    Imputation de '{col}' terminée au niveau {i+1}.\")\n",
    "                    break\n",
    "            \n",
    "            # Imputation finale pour les valeurs encore manquantes\n",
    "            if missing_mask.any():\n",
    "                median_value = df_imputed[col].median()\n",
    "                df_imputed.loc[missing_mask, col] = median_value\n",
    "                print(f\"  Imputation globale des {missing_mask.sum()} valeurs restantes avec la médiane: {median_value}\")\n",
    "    \n",
    "    # Méthode 2: Imputation par régression linéaire\n",
    "    elif method == 'regression':\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        \n",
    "        for col in ['year', 'age']:\n",
    "            if (col == 'year' and impute_year and has_year) or (col == 'age' and impute_age and has_age):\n",
    "                missing_mask = df_imputed[col].isna()\n",
    "                total_missing = missing_mask.sum()\n",
    "                \n",
    "                if total_missing == 0:\n",
    "                    print(f\"Aucune valeur manquante dans '{col}'.\")\n",
    "                    continue\n",
    "                \n",
    "                print(f\"Imputation de {total_missing} valeurs manquantes dans '{col}' par régression...\")\n",
    "                \n",
    "                # Sélectionner les colonnes numériques pour la régression\n",
    "                numeric_cols = df_imputed.select_dtypes(include=['number']).columns\n",
    "                numeric_cols = [c for c in numeric_cols if c != col and df_imputed[c].isna().sum() == 0]\n",
    "                \n",
    "                if len(numeric_cols) < 2:\n",
    "                    print(f\"  Pas assez de variables numériques pour la régression. Utilisation de la médiane.\")\n",
    "                    df_imputed.loc[missing_mask, col] = df_imputed[col].median()\n",
    "                    continue\n",
    "                \n",
    "                # Création des ensembles d'entraînement\n",
    "                X_train = df_imputed.loc[~missing_mask, numeric_cols]\n",
    "                y_train = df_imputed.loc[~missing_mask, col]\n",
    "                \n",
    "                # Entraîner le modèle\n",
    "                model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "                model.fit(X_train, y_train)\n",
    "                \n",
    "                # Prédire les valeurs manquantes\n",
    "                X_missing = df_imputed.loc[missing_mask, numeric_cols]\n",
    "                y_pred = model.predict(X_missing)\n",
    "                \n",
    "                # Imputer les valeurs prédites\n",
    "                df_imputed.loc[missing_mask, col] = y_pred\n",
    "                \n",
    "                print(f\"  {total_missing} valeurs imputées dans '{col}' par régression.\")\n",
    "    \n",
    "    # Méthode 3: Imputation par KNN\n",
    "    elif method == 'knn':\n",
    "        from sklearn.impute import KNNImputer\n",
    "        \n",
    "        # Sélectionner les colonnes numériques pour l'imputation KNN\n",
    "        numeric_cols = df_imputed.select_dtypes(include=['number']).columns.tolist()\n",
    "        \n",
    "        # Filtrer uniquement les colonnes avec peu ou pas de valeurs manquantes\n",
    "        valid_cols = [col for col in numeric_cols if df_imputed[col].isna().mean() < 0.3]\n",
    "        \n",
    "        if len(valid_cols) < 3:\n",
    "            print(\"Pas assez de variables numériques pour l'imputation KNN. Utilisation de la médiane.\")\n",
    "            \n",
    "            if impute_year and has_year:\n",
    "                df_imputed['year'].fillna(df_imputed['year'].median(), inplace=True)\n",
    "            \n",
    "            if impute_age and has_age:\n",
    "                df_imputed['age'].fillna(df_imputed['age'].median(), inplace=True)\n",
    "        else:\n",
    "            print(f\"Imputation KNN avec {len(valid_cols)} variables numériques...\")\n",
    "            \n",
    "            # Créer un sous-ensemble des données numériques\n",
    "            numeric_data = df_imputed[valid_cols].copy()\n",
    "            \n",
    "            # Appliquer l'imputation KNN\n",
    "            imputer = KNNImputer(n_neighbors=5)\n",
    "            imputed_values = imputer.fit_transform(numeric_data)\n",
    "            \n",
    "            # Reconstruire le DataFrame avec les valeurs imputées\n",
    "            numeric_df_imputed = pd.DataFrame(imputed_values, columns=valid_cols, index=df_imputed.index)\n",
    "            \n",
    "            # Remplacer uniquement les valeurs manquantes dans les colonnes cibles\n",
    "            if impute_year and has_year and 'year' in valid_cols:\n",
    "                missing_mask = df_imputed['year'].isna()\n",
    "                df_imputed.loc[missing_mask, 'year'] = numeric_df_imputed.loc[missing_mask, 'year']\n",
    "                print(f\"Imputé {missing_mask.sum()} valeurs manquantes dans 'year' avec KNN.\")\n",
    "            \n",
    "            if impute_age and has_age and 'age' in valid_cols:\n",
    "                missing_mask = df_imputed['age'].isna()\n",
    "                df_imputed.loc[missing_mask, 'age'] = numeric_df_imputed.loc[missing_mask, 'age']\n",
    "                print(f\"Imputé {missing_mask.sum()} valeurs manquantes dans 'age' avec KNN.\")\n",
    "    \n",
    "    # 3. Vérification de cohérence après imputation\n",
    "    if has_year and has_age:\n",
    "        # S'assurer que year + age = année actuelle (approximativement)\n",
    "        tolerance = 3  # Tolérance de 3 ans\n",
    "        inconsistent_mask = abs((df_imputed['construction_year'] + df_imputed['age']) - current_year) > tolerance\n",
    "        \n",
    "        if inconsistent_mask.any():\n",
    "            print(f\"Attention: {inconsistent_mask.sum()} propriétés ont des valeurs d'année et d'âge incohérentes après imputation.\")\n",
    "    \n",
    "    # 4. Arrondir l'année à l'entier le plus proche\n",
    "    if has_year:\n",
    "        df_imputed['construction_year'] = df_imputed['construction_year'].round().astype('Int64')\n",
    "    \n",
    "    if has_age:\n",
    "        df_imputed['age'] = df_imputed['age'].round().astype('Int64')\n",
    "    \n",
    "    # Nettoyer les colonnes temporaires\n",
    "    if 'price_range' in df_imputed.columns and 'price_range' not in df.columns:\n",
    "        df_imputed.drop('price_range', axis=1, inplace=True)\n",
    "    \n",
    "    return df_imputed\n",
    "\n",
    "# Exemple d'utilisation\n",
    "df_imputed = impute_property_year_age(df, method='grouped_median')\n",
    "df_imputed['construction_year']=2025-df_imputed['age']\n",
    "# Pour utiliser la méthode de régression (meilleure pour les grandes bases de données)\n",
    "# df_imputed = impute_property_year_age(df, method='regression')\n",
    "\n",
    "# Pour utiliser la méthode KNN\n",
    "# df_imputed = impute_property_year_age(df, method='knn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cf05ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age']=df_imputed['age']\n",
    "df['construction_year']=df_imputed['construction_year']\n",
    "missing_summary_after_imputation = analyze_missing_data(df)\n",
    "missing_summary_after_imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03483549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputer les valeurs manquantes dans les colonnes binaires représentant les équipements immobiliers\n",
    "def impute_binary_amenities(df, binary_columns=None, grouping_columns=['city', 'property_type', 'transaction']):\n",
    "    \"\"\"\n",
    "    Impute les valeurs manquantes dans les colonnes binaires représentant les équipements immobiliers.\n",
    "    Utilise une approche par niveaux basée sur les colonnes de groupement.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame contenant les données immobilières\n",
    "    binary_columns : list\n",
    "        Liste des colonnes binaires à imputer\n",
    "    grouping_columns : list\n",
    "        Liste des colonnes à utiliser pour le groupement (par défaut: city, property_type, transaction)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame avec les valeurs imputées\n",
    "    \"\"\"\n",
    "    # Créer une copie pour ne pas modifier le dataframe original\n",
    "    df_imputed = df.copy()\n",
    "    \n",
    "    # Vérifier les colonnes binaires à imputer\n",
    "    if binary_columns is None:\n",
    "        print(\"Aucune colonne binaire spécifiée pour l'imputation\")\n",
    "        return df_imputed\n",
    "    \n",
    "    # Filtrer les colonnes existantes\n",
    "    binary_columns = [col for col in binary_columns if col in df_imputed.columns]\n",
    "    grouping_columns = [col for col in grouping_columns if col in df_imputed.columns]\n",
    "    \n",
    "    print(f\"Colonnes binaires à imputer: {', '.join(binary_columns)}\")\n",
    "    print(f\"Colonnes de groupement: {', '.join(grouping_columns)}\")\n",
    "    \n",
    "    # Générer toutes les combinaisons de colonnes de groupement\n",
    "    import itertools\n",
    "    grouping_combinations = []\n",
    "    \n",
    "    # Ajouter les combinaisons de colonnes de groupement, du plus spécifique au plus général\n",
    "    for i in range(len(grouping_columns), 0, -1):\n",
    "        grouping_combinations.extend(list(itertools.combinations(grouping_columns, i)))\n",
    "    \n",
    "    # Pour chaque colonne binaire à imputer\n",
    "    for col in binary_columns:\n",
    "        # Vérifier s'il y a des valeurs manquantes\n",
    "        missing_mask = df_imputed[col].isna()\n",
    "        missing_count = missing_mask.sum()\n",
    "        \n",
    "        if missing_count == 0:\n",
    "            print(f\"- {col}: Aucune valeur manquante\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"- {col}: Imputation de {missing_count} valeurs manquantes\")\n",
    "        \n",
    "        # Imputation par niveaux\n",
    "        for level, group_cols in enumerate(grouping_combinations):\n",
    "            if not missing_mask.any():\n",
    "                break\n",
    "                \n",
    "            # Calculer le mode (valeur la plus fréquente) par groupe\n",
    "            group_modes = df_imputed[~missing_mask].groupby(list(group_cols))[col].agg(\n",
    "                lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else None\n",
    "            )\n",
    "            \n",
    "            # Imputer les valeurs manquantes par groupe\n",
    "            for index, row in df_imputed[missing_mask].iterrows():\n",
    "                try:\n",
    "                    # Créer la clé de groupe\n",
    "                    group_key = tuple(row[gc] for gc in group_cols)\n",
    "                    \n",
    "                    # Imputer si le mode existe pour ce groupe\n",
    "                    if group_key in group_modes.index and group_modes[group_key] is not None:\n",
    "                        df_imputed.loc[index, col] = group_modes[group_key]\n",
    "                except:\n",
    "                    # Ignorer les erreurs (ex: valeurs manquantes dans les colonnes de groupement)\n",
    "                    continue\n",
    "            \n",
    "            # Mettre à jour le masque des valeurs manquantes\n",
    "            new_missing_mask = df_imputed[col].isna()\n",
    "            imputed_in_level = missing_mask.sum() - new_missing_mask.sum()\n",
    "            \n",
    "            if imputed_in_level > 0:\n",
    "                print(f\"  Niveau {level+1} ({', '.join(group_cols)}): {imputed_in_level} valeurs imputées\")\n",
    "            \n",
    "            missing_mask = new_missing_mask\n",
    "        \n",
    "        # Imputation finale avec le mode global pour les valeurs restantes\n",
    "        if missing_mask.any():\n",
    "            global_mode = df_imputed[col].mode().iloc[0]\n",
    "            df_imputed.loc[missing_mask, col] = global_mode\n",
    "            print(f\"  Imputation globale: {missing_mask.sum()} valeurs imputées avec {global_mode}\")\n",
    "    \n",
    "    return df_imputed\n",
    "\n",
    "# Utilisation de la fonction\n",
    "binary_cols = ['swimming_pool', 'central_heating', 'air_conditioning', \n",
    "               'elevator', 'garden', 'equipped_kitchen']\n",
    "\n",
    "df_imputed = impute_binary_amenities(\n",
    "    df, \n",
    "    binary_columns=binary_cols,\n",
    "    grouping_columns=['city', 'property_type', 'transaction']\n",
    ")\n",
    "df['swimming_pool'] = df_imputed['swimming_pool']\n",
    "df['central_heating'] = df_imputed['central_heating']\n",
    "df['air_conditioning'] = df_imputed['air_conditioning']\n",
    "df['elevator'] = df_imputed['elevator']\n",
    "df['garden'] = df_imputed['garden']\n",
    "df['equipped_kitchen'] = df_imputed['equipped_kitchen']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522b3f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['amenities'], inplace=True)\n",
    "missing_summary_after_imputation = analyze_missing_data(df)\n",
    "missing_summary_after_imputation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4435653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputation des valeurs manquantes dans les colonnes représentant le nombre de pièces, chambres, salles de bain et parkings en fonction de la superficie et du type de propriété\n",
    "def simple_impute_rooms(df, rooms_col='rooms', area_col='size', property_type_col='property_type'):\n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame contenant les données immobilières\n",
    "    rooms_col : str\n",
    "        Nom de la colonne contenant le nombre de pièces/ nombre de parkings/ nombre de chambres/ salles de bain\n",
    "        (ex: 'rooms', 'bedrooms', 'bathrooms', 'parkings')\n",
    "    area_col : str\n",
    "        Nom de la colonne contenant la superficie\n",
    "    property_type_col : str\n",
    "        Nom de la colonne contenant le type de propriété\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame avec les valeurs de rooms imputées\n",
    "    \"\"\"\n",
    "    # Copie du dataframe\n",
    "    df_imputed = df.copy()\n",
    "    \n",
    "    # Calculer le nombre de valeurs manquantes\n",
    "    missing_count = df_imputed[rooms_col].isna().sum()\n",
    "    print(f\"Imputation de {missing_count} valeurs manquantes dans '{rooms_col}'\")\n",
    "    \n",
    "    # Créer des segments de superficie (quartiles)\n",
    "    area_bins = [0] + list(df_imputed[area_col].quantile([0.25, 0.5, 0.75, 1.0]))\n",
    "    df_imputed['area_segment'] = pd.cut(df_imputed[area_col], bins=area_bins, include_lowest=True)\n",
    "    \n",
    "    # Méthode principale: calculer le ratio moyen pièces/superficie par type de propriété\n",
    "    # Cela donne une idée de combien de m² par pièce selon le type de logement\n",
    "    \n",
    "    # Calculer le ratio moyen pièces/superficie pour chaque type de propriété\n",
    "    ratios = df_imputed.dropna(subset=[rooms_col]).groupby(property_type_col).apply(\n",
    "        lambda x: (x[rooms_col] / x[area_col]).median()\n",
    "    ).to_dict()\n",
    "    \n",
    "    # Imputer les valeurs manquantes directement\n",
    "    missing_mask = df_imputed[rooms_col].isna()\n",
    "    \n",
    "    for prop_type in ratios:\n",
    "        # Pour chaque type de propriété, imputer en fonction du ratio\n",
    "        type_mask = (df_imputed[property_type_col] == prop_type) & missing_mask\n",
    "        if type_mask.any():\n",
    "            # Estimer le nombre de pièces en fonction de la superficie et du ratio\n",
    "            df_imputed.loc[type_mask, rooms_col] = (df_imputed.loc[type_mask, area_col] * ratios[prop_type]).round()\n",
    "    \n",
    "    # Imputer les valeurs restantes par segment de superficie\n",
    "    still_missing = df_imputed[rooms_col].isna()\n",
    "    if still_missing.any():\n",
    "        # Calculer le nombre moyen de pièces par segment de superficie\n",
    "        segment_means = df_imputed.groupby('area_segment')[rooms_col].transform(\n",
    "            lambda x: x.median() if not x.dropna().empty else None\n",
    "        )\n",
    "        \n",
    "        # Imputer les valeurs manquantes\n",
    "        df_imputed.loc[still_missing, rooms_col] = segment_means.loc[still_missing]\n",
    "    \n",
    "    # Imputer les dernières valeurs manquantes avec la médiane globale\n",
    "    final_missing = df_imputed[rooms_col].isna()\n",
    "    if final_missing.any():\n",
    "        median_rooms = df_imputed[rooms_col].median()\n",
    "        df_imputed.loc[final_missing, rooms_col] = round(median_rooms)\n",
    "    \n",
    "    # Arrondir à l'entier le plus proche\n",
    "    df_imputed[rooms_col] = df_imputed[rooms_col].round()\n",
    "    \n",
    "    # Supprimer la colonne temporaire\n",
    "    df_imputed.drop('area_segment', axis=1, inplace=True)\n",
    "    \n",
    "    print(f\"Imputation terminée.\")\n",
    "    \n",
    "    return df_imputed\n",
    "\n",
    "df=simple_impute_rooms(df, rooms_col='rooms', area_col='size', property_type_col='property_type')\n",
    "df=simple_impute_rooms(df, rooms_col='bedrooms', area_col='size', property_type_col='property_type')\n",
    "df=simple_impute_rooms(df, rooms_col='bathrooms', area_col='size', property_type_col='property_type')\n",
    "df=simple_impute_rooms(df, rooms_col='parkings', area_col='size', property_type_col='property_type')\n",
    "\n",
    "# Vérification des valeurs manquantes restantes\n",
    "missing_summary_after_imputation = analyze_missing_data(df)\n",
    "missing_summary_after_imputation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab8104b",
   "metadata": {},
   "source": [
    "# Régression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c36eb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def prepare_data_for_regression(df):\n",
    "    \"\"\"\n",
    "    Prépare les données pour la régression - encode uniquement condition, finishing et variables binaires\n",
    "    \"\"\"\n",
    "    df_prep = df.copy()\n",
    "    \n",
    "    # Traitement des variables ordinales\n",
    "    # Définir l'ordre pour chaque variable ordinale\n",
    "    condition_categories = ['à rénover', 'à rafraichir', 'bonne condition', 'excellente condition', 'neuf']  \n",
    "    finishing_categories = ['social', 'économique', 'moyen standing', 'haut standing', 'très haut standing']  \n",
    "    \n",
    "    # Encoder les variables ordinales\n",
    "    if 'condition' in df_prep.columns:\n",
    "        cat_map = {cat: i for i, cat in enumerate(condition_categories)}\n",
    "        df_prep['condition'] = df_prep['condition'].map(cat_map)\n",
    "        df_prep['condition'].fillna(df_prep['condition'].median(), inplace=True)\n",
    "    \n",
    "    if 'finishing' in df_prep.columns:\n",
    "        cat_map = {cat: i for i, cat in enumerate(finishing_categories)}\n",
    "        df_prep['finishing'] = df_prep['finishing'].map(cat_map)\n",
    "        df_prep['finishing'].fillna(df_prep['finishing'].median(), inplace=True)\n",
    "    \n",
    "    # S'assurer que les variables binaires sont numériques\n",
    "    binary_cols = [col for col in df_prep.columns if df_prep[col].nunique() == 2 and \n",
    "                  col not in ['transaction', 'property_type', 'city', 'neighborhood']]\n",
    "    \n",
    "    for col in binary_cols:\n",
    "        if df_prep[col].dtype != 'int64' and df_prep[col].dtype != 'float64':\n",
    "            df_prep[col] = df_prep[col].astype(int)\n",
    "    \n",
    "    return df_prep\n",
    "\n",
    "def regression_par_segment(df, city=None, property_type=None, transaction=None, target_column='price'):\n",
    "    \"\"\"\n",
    "    Réalise une régression linéaire simple sur un segment spécifique des données\n",
    "    \n",
    "    Paramètres:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        Le dataframe déjà préparé avec prepare_data_for_regression\n",
    "    city : str, optional\n",
    "        Ville à filtrer\n",
    "    property_type : str, optional\n",
    "        Type de propriété à filtrer\n",
    "    transaction : str, optional\n",
    "        Type de transaction à filtrer\n",
    "    target_column : str\n",
    "        Nom de la colonne cible\n",
    "    \n",
    "    Retourne:\n",
    "    ---------\n",
    "    model : objet modèle\n",
    "        Le modèle de régression linéaire\n",
    "    feature_importance : DataFrame\n",
    "        Importance des caractéristiques\n",
    "    metrics : dict\n",
    "        Métriques de performance\n",
    "    \"\"\"\n",
    "    # Filtrer les données selon les paramètres\n",
    "    df_filtered = df.copy()\n",
    "    \n",
    "    if city is not None and 'city' in df_filtered.columns:\n",
    "        df_filtered = df_filtered[df_filtered['city'] == city]\n",
    "        df_filtered = df_filtered.drop(columns=['city']) \n",
    "    \n",
    "    if property_type is not None and 'property_type' in df_filtered.columns:\n",
    "        df_filtered = df_filtered[df_filtered['property_type'] == property_type]\n",
    "        df_filtered = df_filtered.drop(columns=['property_type'])\n",
    "    \n",
    "    if transaction is not None and 'transaction' in df_filtered.columns:\n",
    "        df_filtered = df_filtered[df_filtered['transaction'] == transaction]\n",
    "        df_filtered = df_filtered.drop(columns=['transaction'])\n",
    "    \n",
    "    # Supprimer les colonnes non nécessaires pour la régression\n",
    "    columns_to_drop = ['date', 'source', 'neighborhood', 'suffix','listing_price','price_ttc']\n",
    "    columns_to_drop = [col for col in columns_to_drop if col in df_filtered.columns]\n",
    "    if columns_to_drop:\n",
    "        df_filtered = df_filtered.drop(columns=columns_to_drop)\n",
    "    \n",
    "    # Supprimer les lignes avec des valeurs manquantes dans la colonne cible\n",
    "    df_filtered = df_filtered.dropna(subset=[target_column])\n",
    "    \n",
    "    # Séparer les caractéristiques et la cible\n",
    "    y = df_filtered[target_column]\n",
    "    X = df_filtered.drop(columns=[target_column])\n",
    "    \n",
    "    # Exclure les colonnes non numériques\n",
    "    X = X.select_dtypes(include=['number'])\n",
    "    \n",
    "    # Diviser en ensembles d'entraînement et de test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Normaliser les caractéristiques\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Entraîner le modèle\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Prédictions\n",
    "    y_train_pred = model.predict(X_train_scaled)\n",
    "    y_test_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Évaluer le modèle\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    \n",
    "    # Importance des caractéristiques\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Caractéristique': X.columns,\n",
    "        'Coefficient': model.coef_\n",
    "    }).sort_values('Coefficient', key=abs, ascending=False)\n",
    "    \n",
    "    # Afficher les résultats\n",
    "    print(f\"Nombre d'observations: {len(df_filtered)}\")\n",
    "    print(f\"R² (entraînement): {train_r2:.4f}\")\n",
    "    print(f\"R² (test): {test_r2:.4f}\")\n",
    "    print(f\"RMSE (test): {test_rmse:.2f}\")\n",
    "    print(f\"MAE (test): {test_mae:.2f}\")\n",
    "    print(\"\\nTop caractéristiques les plus influentes:\")\n",
    "    print(feature_importance.head(10))\n",
    "    \n",
    "    # Préparer visualisation\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test, y_test_pred, alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    plt.xlabel('Prix réels')\n",
    "    plt.ylabel('Prix prédits')\n",
    "    plt.title('Prix réels vs Prix prédits')\n",
    "    plt.grid(True)\n",
    "    plt.annotate(f'R² = {test_r2:.4f}', xy=(0.05, 0.95), xycoords='axes fraction', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Graphique d'importance des caractéristiques\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    top_features = feature_importance.head(10)\n",
    "    colors = ['green' if coef > 0 else 'red' for coef in top_features['Coefficient']]\n",
    "    plt.barh(top_features['Caractéristique'], top_features['Coefficient'], color=colors)\n",
    "    plt.xlabel('Valeur du coefficient')\n",
    "    plt.ylabel('Caractéristique')\n",
    "    plt.title('Top 10 des caractéristiques les plus importantes')\n",
    "    plt.axvline(x=0, color='k', linestyle='--')\n",
    "    plt.grid(True, axis='x')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    metrics = {\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'test_rmse': test_rmse,\n",
    "        'test_mae': test_mae\n",
    "    }\n",
    "    \n",
    "    return model, feature_importance, metrics\n",
    "\n",
    "# Exemple d'utilisation:\n",
    "# 1. Préparer les données\n",
    "df_prep = prepare_data_for_regression(df)\n",
    "#\n",
    "# 2. Réaliser une régression pour un segment spécifique\n",
    "model, importance, metrics = regression_par_segment(\n",
    "    df_prep,\n",
    "    city='Cite El Khadra', \n",
    "    property_type='bureau',\n",
    "    transaction='rent'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de89cead",
   "metadata": {},
   "source": [
    "## Technique d'apprentissage supervisé : Foret aléatoire\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cbef52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "def random_forest_par_segment(df, city=None, property_type=None, transaction=None, \n",
    "                             target_column='price', n_estimators=100, max_depth=None):\n",
    "    \"\"\"\n",
    "    Réalise une régression par Random Forest sur un segment spécifique des données\n",
    "    \n",
    "    Paramètres:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        Le dataframe déjà préparé avec prepare_data_for_regression\n",
    "    city : str, optional\n",
    "        Ville à filtrer\n",
    "    property_type : str, optional\n",
    "        Type de propriété à filtrer\n",
    "    transaction : str, optional\n",
    "        Type de transaction à filtrer\n",
    "    target_column : str\n",
    "        Nom de la colonne cible\n",
    "    n_estimators : int\n",
    "        Nombre d'arbres dans la forêt\n",
    "    max_depth : int, optional\n",
    "        Profondeur maximale des arbres\n",
    "    \n",
    "    Retourne:\n",
    "    ---------\n",
    "    model : objet modèle\n",
    "        Le modèle Random Forest\n",
    "    feature_importance : DataFrame\n",
    "        Importance des caractéristiques\n",
    "    metrics : dict\n",
    "        Métriques de performance\n",
    "    \"\"\"\n",
    "    # Filtrer les données selon les paramètres\n",
    "    df_filtered = df.copy()\n",
    "    \n",
    "    if city is not None and 'city' in df_filtered.columns:\n",
    "        df_filtered = df_filtered[df_filtered['city'] == city]\n",
    "        df_filtered = df_filtered.drop(columns=['city']) \n",
    "    \n",
    "    if property_type is not None and 'property_type' in df_filtered.columns:\n",
    "        df_filtered = df_filtered[df_filtered['property_type'] == property_type]\n",
    "        df_filtered = df_filtered.drop(columns=['property_type'])\n",
    "    \n",
    "    if transaction is not None and 'transaction' in df_filtered.columns:\n",
    "        df_filtered = df_filtered[df_filtered['transaction'] == transaction]\n",
    "        df_filtered = df_filtered.drop(columns=['transaction'])\n",
    "    \n",
    "    # Supprimer les colonnes non nécessaires pour la régression\n",
    "    columns_to_drop = ['listing_price','price_ttc','date', 'source', 'neighborhood', 'suffix']\n",
    "    columns_to_drop = [col for col in columns_to_drop if col in df_filtered.columns]\n",
    "    if columns_to_drop:\n",
    "        df_filtered = df_filtered.drop(columns=columns_to_drop)\n",
    "    \n",
    "    # Supprimer les lignes avec des valeurs manquantes dans la colonne cible\n",
    "    df_filtered = df_filtered.dropna(subset=[target_column])\n",
    "    \n",
    "    # Séparer les caractéristiques et la cible\n",
    "    y = df_filtered[target_column]\n",
    "    X = df_filtered.drop(columns=[target_column])\n",
    "    \n",
    "    # Exclure les colonnes non numériques\n",
    "    X = X.select_dtypes(include=['number'])\n",
    "    \n",
    "    # Diviser en ensembles d'entraînement et de test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Normaliser les caractéristiques (facultatif pour Random Forest)\n",
    "    # On le fait pour rester cohérent avec la régression linéaire\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Créer et entraîner le modèle Random Forest\n",
    "    rf_model = RandomForestRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        random_state=42,\n",
    "        n_jobs=-1  # Utiliser tous les cœurs disponibles\n",
    "    )\n",
    "    rf_model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Prédictions\n",
    "    y_train_pred = rf_model.predict(X_train_scaled)\n",
    "    y_test_pred = rf_model.predict(X_test_scaled)\n",
    "    \n",
    "    # Évaluer le modèle\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    \n",
    "    # Importance des caractéristiques\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Caractéristique': X.columns,\n",
    "        'Importance': rf_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Afficher les résultats\n",
    "    print(f\"===== RÉSULTATS RANDOM FOREST =====\")\n",
    "    print(f\"Segments: Ville={city}, Type={property_type}, Transaction={transaction}\")\n",
    "    print(f\"Nombre d'observations: {len(df_filtered)}\")\n",
    "    print(f\"R² (entraînement): {train_r2:.4f}\")\n",
    "    print(f\"R² (test): {test_r2:.4f}\")\n",
    "    print(f\"RMSE (test): {test_rmse:.2f}\")\n",
    "    print(f\"MAE (test): {test_mae:.2f}\")\n",
    "    print(\"\\nTop caractéristiques les plus importantes:\")\n",
    "    print(feature_importance.head(10))\n",
    "    \n",
    "    # Préparer visualisation\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test, y_test_pred, alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    plt.xlabel('Prix réels')\n",
    "    plt.ylabel('Prix prédits')\n",
    "    plt.title('Random Forest: Prix réels vs Prix prédits')\n",
    "    plt.grid(True)\n",
    "    plt.annotate(f'R² = {test_r2:.4f}', xy=(0.05, 0.95), xycoords='axes fraction', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Graphique d'importance des caractéristiques\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    top_features = feature_importance.head(10)\n",
    "    sns.barplot(x='Importance', y='Caractéristique', data=top_features, palette='viridis')\n",
    "    plt.xlabel('Importance relative (%)')\n",
    "    plt.ylabel('Caractéristique')\n",
    "    plt.title('Random Forest: Top 10 des caractéristiques les plus importantes')\n",
    "    plt.grid(True, axis='x')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualiser les résidus\n",
    "    residus = y_test - y_test_pred\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test_pred, residus, alpha=0.5)\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.xlabel('Prix prédits')\n",
    "    plt.ylabel('Résidus')\n",
    "    plt.title('Random Forest: Distribution des résidus')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    metrics = {\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'test_rmse': test_rmse,\n",
    "        'test_mae': test_mae\n",
    "    }\n",
    "    \n",
    "    return rf_model, feature_importance, metrics\n",
    "\n",
    "\n",
    "def comparer_modeles(df, city=None, property_type=None, transaction=None, target_column='price'):\n",
    "    \"\"\"\n",
    "    Compare les performances de la régression linéaire et du Random Forest\n",
    "    \"\"\"\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    \n",
    "    # Filtrer les données\n",
    "    df_filtered = df.copy()\n",
    "    \n",
    "    if city is not None and 'city' in df_filtered.columns:\n",
    "        df_filtered = df_filtered[df_filtered['city'] == city]\n",
    "        df_filtered = df_filtered.drop(columns=['city']) \n",
    "    \n",
    "    if property_type is not None and 'property_type' in df_filtered.columns:\n",
    "        df_filtered = df_filtered[df_filtered['property_type'] == property_type]\n",
    "        df_filtered = df_filtered.drop(columns=['property_type'])\n",
    "    \n",
    "    if transaction is not None and 'transaction' in df_filtered.columns:\n",
    "        df_filtered = df_filtered[df_filtered['transaction'] == transaction]\n",
    "        df_filtered = df_filtered.drop(columns=['transaction'])\n",
    "    \n",
    "    # Nettoyer le dataframe\n",
    "    columns_to_drop = ['date', 'source', 'neighborhood', 'suffix']\n",
    "    columns_to_drop = [col for col in columns_to_drop if col in df_filtered.columns]\n",
    "    if columns_to_drop:\n",
    "        df_filtered = df_filtered.drop(columns=columns_to_drop)\n",
    "    \n",
    "    df_filtered = df_filtered.dropna(subset=[target_column])\n",
    "    \n",
    "    # Séparer X et y\n",
    "    y = df_filtered[target_column]\n",
    "    X = df_filtered.drop(columns=[target_column])\n",
    "    X = X.select_dtypes(include=['number'])\n",
    "    \n",
    "    # Diviser en ensembles d'entraînement et de test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Normaliser les caractéristiques\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Modèle 1: Régression linéaire\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train_scaled, y_train)\n",
    "    lr_train_pred = lr.predict(X_train_scaled)\n",
    "    lr_test_pred = lr.predict(X_test_scaled)\n",
    "    lr_train_r2 = r2_score(y_train, lr_train_pred)\n",
    "    lr_test_r2 = r2_score(y_test, lr_test_pred)\n",
    "    lr_rmse = np.sqrt(mean_squared_error(y_test, lr_test_pred))\n",
    "    lr_mae = mean_absolute_error(y_test, lr_test_pred)\n",
    "    \n",
    "    # Modèle 2: Random Forest\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    rf.fit(X_train_scaled, y_train)\n",
    "    rf_train_pred = rf.predict(X_train_scaled)\n",
    "    rf_test_pred = rf.predict(X_test_scaled)\n",
    "    rf_train_r2 = r2_score(y_train, rf_train_pred)\n",
    "    rf_test_r2 = r2_score(y_test, rf_test_pred)\n",
    "    rf_rmse = np.sqrt(mean_squared_error(y_test, rf_test_pred))\n",
    "    rf_mae = mean_absolute_error(y_test, rf_test_pred)\n",
    "    \n",
    "    # Afficher les résultats\n",
    "    print(f\"===== COMPARAISON DES MODÈLES =====\")\n",
    "    print(f\"Segments: Ville={city}, Type={property_type}, Transaction={transaction}\")\n",
    "    print(f\"Nombre d'observations: {len(df_filtered)}\")\n",
    "    \n",
    "    # Créer un dataframe de comparaison\n",
    "    comparison = pd.DataFrame({\n",
    "        'Modèle': ['Régression Linéaire', 'Random Forest'],\n",
    "        'R² (train)': [lr_train_r2, rf_train_r2],\n",
    "        'R² (test)': [lr_test_r2, rf_test_r2],\n",
    "        'RMSE': [lr_rmse, rf_rmse],\n",
    "        'MAE': [lr_mae, rf_mae]\n",
    "    })\n",
    "    \n",
    "    print(comparison)\n",
    "    \n",
    "    # Visualisation\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Graphique 1: Comparaison des R²\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.barplot(x='Modèle', y='R² (test)', data=comparison, palette='viridis')\n",
    "    plt.title('Comparaison du R²')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.grid(True, axis='y')\n",
    "    \n",
    "    # Graphique 2: Comparaison des RMSE\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.barplot(x='Modèle', y='RMSE', data=comparison, palette='viridis')\n",
    "    plt.title('Comparaison du RMSE')\n",
    "    plt.grid(True, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Comparaison des prédictions\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    # Graphique 1: Régression Linéaire\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(y_test, lr_test_pred, alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    plt.xlabel('Prix réels')\n",
    "    plt.ylabel('Prix prédits')\n",
    "    plt.title(f'Régression Linéaire: R² = {lr_test_r2:.4f}')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Graphique 2: Random Forest\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(y_test, rf_test_pred, alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    plt.xlabel('Prix réels')\n",
    "    plt.ylabel('Prix prédits')\n",
    "    plt.title(f'Random Forest: R² = {rf_test_r2:.4f}')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return comparison\n",
    "\n",
    "# Exemple d'utilisation:\n",
    "# 1. Préparer les données\n",
    "# df_prep = prepare_data_for_regression(df)\n",
    "#\n",
    "# 2. Réaliser une régression avec Random Forest pour un segment spécifique\n",
    "rf_model, rf_importance, rf_metrics = random_forest_par_segment(\n",
    "    df_prep,\n",
    "    city='La Soukra', \n",
    "    property_type='villa',\n",
    "    transaction='sale'\n",
    ")\n",
    "#\n",
    "# 3. Optimiser les hyperparamètres (optionnel)\n",
    "\n",
    "#\n",
    "# 4. Comparer avec la régression linéaire\n",
    "comparison = comparer_modeles(\n",
    "     df_prep,\n",
    "    city='La Soukra', \n",
    "    property_type='appartement',\n",
    "    transaction='sale'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb1d9d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e40e9301",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516f40ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cecc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_simple(df, city=None, property_type=None, transaction=None, target_column='price'):\n",
    "    \"\"\"\n",
    "    Fonction simple pour appliquer XGBoost à un segment spécifique\n",
    "    \n",
    "    Paramètres:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        Le dataframe préparé\n",
    "    city : str, optional\n",
    "        Ville à filtrer\n",
    "    property_type : str, optional\n",
    "        Type de propriété à filtrer\n",
    "    transaction : str, optional\n",
    "        Type de transaction à filtrer\n",
    "    target_column : str\n",
    "        Nom de la colonne cible\n",
    "    \"\"\"\n",
    "    # Filtrer les données selon les paramètres\n",
    "    df_filtered = df.copy()\n",
    "    \n",
    "    if city is not None and 'city' in df_filtered.columns:\n",
    "        df_filtered = df_filtered[df_filtered['city'] == city]\n",
    "        df_filtered = df_filtered.drop(columns=['city']) \n",
    "    \n",
    "    if property_type is not None and 'property_type' in df_filtered.columns:\n",
    "        df_filtered = df_filtered[df_filtered['property_type'] == property_type]\n",
    "        df_filtered = df_filtered.drop(columns=['property_type'])\n",
    "    \n",
    "    if transaction is not None and 'transaction' in df_filtered.columns:\n",
    "        df_filtered = df_filtered[df_filtered['transaction'] == transaction]\n",
    "        df_filtered = df_filtered.drop(columns=['transaction'])\n",
    "    \n",
    "    # Supprimer les colonnes non nécessaires\n",
    "    columns_to_drop = ['listing_price','construction_year','price_ttc','date', 'source', 'neighborhood', 'suffix']\n",
    "    columns_to_drop = [col for col in columns_to_drop if col in df_filtered.columns]\n",
    "    if columns_to_drop:\n",
    "        df_filtered = df_filtered.drop(columns=columns_to_drop)\n",
    "    \n",
    "    # Supprimer les lignes avec des valeurs manquantes dans la colonne cible\n",
    "    df_filtered = df_filtered.dropna(subset=[target_column])\n",
    "    \n",
    "    # Séparer les caractéristiques et la cible\n",
    "    y = df_filtered[target_column]\n",
    "    X = df_filtered.drop(columns=[target_column])\n",
    "    \n",
    "    # Garder uniquement les colonnes numériques\n",
    "    X = X.select_dtypes(include=['number'])\n",
    "    \n",
    "    # Diviser en ensembles d'entraînement et de test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Créer et entraîner le modèle XGBoost avec des paramètres simples\n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=100,   # Nombre d'arbres\n",
    "        learning_rate=0.1,  # Taux d'apprentissage\n",
    "        max_depth=5,        # Profondeur maximale des arbres\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Prédictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Évaluer le modèle\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    \n",
    "    # Afficher les résultats\n",
    "    print(f\"\\nRésultats XGBoost pour: {property_type} - {transaction} - {city}\")\n",
    "    print(f\"Nombre d'observations: {len(df_filtered)}\")\n",
    "    print(f\"R² (entraînement): {train_r2:.4f}\")\n",
    "    print(f\"R² (test): {test_r2:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    \n",
    "    # Importance des caractéristiques\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Caractéristique': X.columns,\n",
    "        'Importance': model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 caractéristiques les plus importantes:\")\n",
    "    print(feature_importance.head(10))\n",
    "    \n",
    "    # Graphique simple des prédictions\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(y_test, y_test_pred, alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    plt.xlabel('Prix réels')\n",
    "    plt.ylabel('Prix prédits')\n",
    "    plt.title(f'XGBoost: Prédictions (R² = {test_r2:.4f})')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Graphique d'importance des caractéristiques\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    top_n = min(10, len(feature_importance))\n",
    "    plt.barh(feature_importance['Caractéristique'].head(top_n), \n",
    "             feature_importance['Importance'].head(top_n))\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Top caractéristiques importantes')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return model, feature_importance, test_r2\n",
    "\n",
    "# Exemple d'utilisation:\n",
    "# df_prep = prepare_data_for_regression(df)\n",
    "model, importance, r2 = xgboost_simple(\n",
    "    df_prep,\n",
    "    city='Cite El Khadra', \n",
    "    property_type='appartement',\n",
    "    transaction='rent'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
