{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4979a8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des bibliothèques nécessaires\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import missingno as msno\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score, calinski_harabasz_score\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import missingno as msno\n",
    "import warnings\n",
    "import importlib; importlib.reload(model_functions)\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('ggplot')\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6119179b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données\n",
    "df=model_functions.read_data('data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885fdc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nStatistiques descriptives:\")\n",
    "display(df.describe().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bb6e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse des données manquantes: \n",
    "model_functions.analyze_missing_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b449ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation des Prix\n",
    "# 1er niveau d'imputation\n",
    "df['price'] = df.groupby(['neighborhood', 'property_type','transaction'])['price'].transform(\n",
    "    lambda x: x.fillna(x.mean())\n",
    ")\n",
    "df['price_ttc'] = df.groupby(['neighborhood', 'property_type','transaction'])['price_ttc'].transform(\n",
    "    lambda x: x.fillna(x.mean())\n",
    ")\n",
    "# 2ème niveau d'imputation\n",
    "df['price'] = df.groupby(['city','transaction'])['price'].transform(lambda x: x.fillna(x.mean()))\n",
    "df['price_ttc'] = df.groupby(['city','transaction'])['price_ttc'].transform(lambda x: x.fillna(x.mean()))\n",
    "df['listing_price'] = df['listing_price'].fillna(df['price']) # Remplir les valeurs manquantes de 'listing_price' avec la valeur de 'price' si disponible\n",
    "df['suffix'] = df['suffix'].fillna('TTC') # remplacer suffixe par ttc par defaut\n",
    "# 3ème niveau d'imputation\n",
    "df = df[df['price'].notnull()] #éliminer les lignes où 'price' est toujours manquant\n",
    "\n",
    "\n",
    "null_price_rows = df[df['price'].isna()]\n",
    "display(null_price_rows)\n",
    "print(f\"Nombre de lignes avec 'price' manquant après imputation : {null_price_rows.shape[0]}\") # de 244 prix manquants on passe à 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea00c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation des variables catégorielles: standing et condition\n",
    "df=model_functions.impute_condition_simple(df)\n",
    "df=model_functions.impute_finishing_simple(df)\n",
    "# Imputation de la variable 'age' et 'construction_year'\n",
    "df=model_functions.impute_property_year_age(df)\n",
    "df['construction_year']=2025-df['age']\n",
    "# Imputation des variables binaires des commodités\n",
    "df=model_functions.impute_binary_amenities(df,['central_heating','air_conditioning','elevator','swimming_pool','equipped_kitchen','garden'])\n",
    "# Imputation des variables numériques des pieces, chambres, salles de bain et parkings\n",
    "df=model_functions.simple_impute_rooms(df)\n",
    "df=model_functions.simple_impute_rooms(df,'bedrooms')\n",
    "df=model_functions.simple_impute_rooms(df,'parkings')\n",
    "df=model_functions.simple_impute_rooms(df,'bathrooms')\n",
    "df.drop(columns=['amenities'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55b907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification de l'imputation\n",
    "model_functions.analyze_missing_data(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b98be0",
   "metadata": {},
   "source": [
    "# Apprentissage supervisé "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c611920f",
   "metadata": {},
   "source": [
    "## Régression linéaire:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d032a511",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     Préparation les données pour la régression - encode uniquement condition, finishing et variables binaires\n",
    "df_regression= model_functions.prepare_data_for_regression(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f292a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Régression linéaire du prix du bien immobilier / segementation par prix/ type de propriete et quartier/ville\n",
    "model, importance, metrics = model_functions.regression_par_segment(\n",
    "    df,\n",
    "    city='Cite El Khadra', \n",
    "    property_type='bureau',\n",
    "    transaction='rent'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbe30e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model, rf_importance, rf_metrics = model_functions.random_forest_par_segment(\n",
    "    df,\n",
    "    city='La Soukra', \n",
    "    property_type='villa',\n",
    "    transaction='sale'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9255411",
   "metadata": {},
   "source": [
    "## Regression logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6b0f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ea80e2",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fbf349",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, importance, r2 = model_functions.xgboost_simple(\n",
    "    df,\n",
    "    city='Cite El Khadra', \n",
    "    property_type='appartement',\n",
    "    transaction='rent'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e5dc40",
   "metadata": {},
   "source": [
    "## Comparaison des résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b93602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = model_functions.comparer_modeles(\n",
    "     df,\n",
    "    city='La Soukra', \n",
    "    property_type='appartement',\n",
    "    transaction='sale'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f26da3d",
   "metadata": {},
   "source": [
    "# Apprentissage non supervisé "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c72db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = df[(df['city'] == 'La Soukra') & (df['property_type'] == 'appartement') & (df['transaction'] == 'sale')]\n",
    "df_scaled, scaler, feature_names = model_functions.prepare_data_for_clustering(filtered)\n",
    "df_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bec4f4b",
   "metadata": {},
   "source": [
    "## PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681395fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPRENDRE LE LIEN ENTRE PCA ET VOS VARIABLES ORIGINALES\n",
    "pca_model, df_pca, explained_variance = model_functions.apply_pca_analysis(df_scaled, 2)\n",
    "# Analyse des liens\n",
    "loadings_df, fig_contrib = model_functions.analyse_complete_pca_variables(pca_model, df_pca, explained_variance, feature_names, df ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6e931d",
   "metadata": {},
   "source": [
    "## Kmeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1c7e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_model, kmeans_n_clusters, kmeans_labels, kmeans_metrics, scores, n_clusters_list = model_functions.apply_kmeans_clustering(\n",
    "    df_scaled, \n",
    "    n_clusters_range=(2, 8),  # Tester de 2 à 8 clusters\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629bd194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. VISUALISATION K-MEANS AVEC MATPLOTLIB\n",
    "# ===================================================================\n",
    "\n",
    "# Créer une figure avec plusieurs sous-graphiques\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Couleurs pour les clusters\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, kmeans_n_clusters))\n",
    "\n",
    "# 1. Clusters en 2D PCA\n",
    "scatter = axes[0,0].scatter(df_pca.iloc[:, 0], df_pca.iloc[:, 1], \n",
    "                           c=kmeans_labels, cmap='Set3', alpha=0.7)\n",
    "axes[0,0].set_xlabel('PC1')\n",
    "axes[0,0].set_ylabel('PC2')\n",
    "axes[0,0].set_title('K-Means - Clusters (2D PCA)')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Ajouter les centres des clusters\n",
    "centers_pca = pca_model.transform(scaler.transform(kmeans_model.cluster_centers_))\n",
    "axes[0,0].scatter(centers_pca[:, 0], centers_pca[:, 1], \n",
    "                 marker='x', s=200, linewidths=3, color='red', label='Centres')\n",
    "axes[0,0].legend()\n",
    "\n",
    "# 2. Distribution des clusters\n",
    "cluster_counts = pd.Series(kmeans_labels).value_counts().sort_index()\n",
    "bars = axes[0,1].bar(range(len(cluster_counts)), cluster_counts.values, \n",
    "                    color=[colors[i] for i in range(len(cluster_counts))])\n",
    "axes[0,1].set_xlabel('Cluster')\n",
    "axes[0,1].set_ylabel('Nombre de propriétés')\n",
    "axes[0,1].set_title('Distribution des clusters K-Means')\n",
    "axes[0,1].set_xticks(range(len(cluster_counts)))\n",
    "axes[0,1].set_xticklabels([f'Cluster {i}' for i in cluster_counts.index])\n",
    "\n",
    "# Ajouter les valeurs sur les barres\n",
    "for i, v in enumerate(cluster_counts.values):\n",
    "    axes[0,1].text(i, v + 0.5, str(v), ha='center', va='bottom')\n",
    "\n",
    "# 3. Évolution du score de silhouette\n",
    "axes[1,0].plot(n_clusters_list, scores, 'bo-', linewidth=2, markersize=8)\n",
    "axes[1,0].axvline(x=kmeans_n_clusters, color='red', linestyle='--', \n",
    "                 label=f'Optimal: {kmeans_n_clusters} clusters')\n",
    "axes[1,0].set_xlabel('Nombre de clusters')\n",
    "axes[1,0].set_ylabel('Score de silhouette')\n",
    "axes[1,0].set_title('Optimisation du nombre de clusters')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "axes[1,0].legend()\n",
    "\n",
    "# 4. Variance expliquée par PCA\n",
    "axes[1,1].bar(range(len(explained_variance)), explained_variance * 100)\n",
    "axes[1,1].set_xlabel('Composante principale')\n",
    "axes[1,1].set_ylabel('Variance expliquée (%)')\n",
    "axes[1,1].set_title('Variance expliquée par PCA')\n",
    "axes[1,1].set_xticks(range(len(explained_variance)))\n",
    "axes[1,1].set_xticklabels([f'PC{i+1}' for i in range(len(explained_variance))])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598d8f05",
   "metadata": {},
   "source": [
    "Cluster 2 = le plus gros groupe (4 propriétés)\n",
    "Cluster 4 = 3 propriétés\n",
    "Clusters 0,1,6,7 = 2 propriétés chacun\n",
    "Clusters 3,5 = 1 propriété chacun (singletons)\n",
    "Problème : Beaucoup de clusters avec très peu de propriétés !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bec30ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# INTERPRÉTATION ET CORRECTION DES RÉSULTATS K-MEANS\n",
    "# ===================================================================\n",
    "\n",
    "print(\"🔍 ANALYSE DES RÉSULTATS K-MEANS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. DIAGNOSTIC DU PROBLÈME\n",
    "print(f\"\\n📊 DIAGNOSTIC:\")\n",
    "print(f\"• Nombre total de propriétés: {len(df_scaled)}\")\n",
    "print(f\"• Nombre de clusters trouvés: {kmeans_n_clusters}\")\n",
    "print(f\"• Ratio propriétés/cluster: {len(df_scaled)/kmeans_n_clusters:.1f}\")\n",
    "\n",
    "# Analyser la distribution des clusters\n",
    "cluster_counts = pd.Series(kmeans_labels).value_counts().sort_index()\n",
    "print(f\"\\n📈 DISTRIBUTION DES CLUSTERS:\")\n",
    "for cluster_id, count in cluster_counts.items():\n",
    "    percentage = (count / len(df_scaled)) * 100\n",
    "    status = \"✅ OK\" if count >= 3 else \"⚠️ TROP PETIT\"\n",
    "    print(f\"Cluster {cluster_id}: {count} propriétés ({percentage:.1f}%) {status}\")\n",
    "\n",
    "# 2. RECOMMANDATION : FORCER 3-4 CLUSTERS\n",
    "print(f\"\\n💡 SOLUTION RECOMMANDÉE:\")\n",
    "print(f\"Forcer un nombre plus raisonnable de clusters (3-4)\")\n",
    "\n",
    "# ===================================================================\n",
    "# REFAIRE K-MEANS AVEC 3 CLUSTERS\n",
    "# ===================================================================\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(\"NOUVEAU K-MEANS AVEC 3 CLUSTERS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Appliquer K-Means avec 3 clusters\n",
    "kmeans_3 = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "labels_3 = kmeans_3.fit_predict(df_scaled)\n",
    "\n",
    "# Calculer les métriques\n",
    "silhouette_3 = silhouette_score(df_scaled, labels_3)\n",
    "\n",
    "print(f\"📊 RÉSULTATS AVEC 3 CLUSTERS:\")\n",
    "print(f\"Score de silhouette: {silhouette_3:.4f}\")\n",
    "\n",
    "# Distribution des nouveaux clusters\n",
    "cluster_counts_3 = pd.Series(labels_3).value_counts().sort_index()\n",
    "print(f\"\\n📈 NOUVELLE DISTRIBUTION:\")\n",
    "for cluster_id, count in cluster_counts_3.items():\n",
    "    percentage = (count / len(df_scaled)) * 100\n",
    "    print(f\"Cluster {cluster_id}: {count} propriétés ({percentage:.1f}%)\")\n",
    "\n",
    "# ===================================================================\n",
    "# VISUALISATION AMÉLIORÉE\n",
    "# ===================================================================\n",
    "print(f\"\\n📊 VISUALISATION COMPARATIVE:\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Graphique 1: K-Means original (8 clusters)\n",
    "scatter1 = axes[0].scatter(df_pca.iloc[:, 0], df_pca.iloc[:, 1], \n",
    "                          c=kmeans_labels, cmap='Set3', alpha=0.7)\n",
    "axes[0].set_xlabel('PC1')\n",
    "axes[0].set_ylabel('PC2')\n",
    "axes[0].set_title(f'K-Means Original: 8 clusters\\n(Silhouette: {kmeans_metrics[\"silhouette_score\"]:.3f})')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Graphique 2: K-Means amélioré (3 clusters)\n",
    "scatter2 = axes[1].scatter(df_pca.iloc[:, 0], df_pca.iloc[:, 1], \n",
    "                          c=labels_3, cmap='viridis', alpha=0.7, s=80)\n",
    "axes[1].set_xlabel('PC1')\n",
    "axes[1].set_ylabel('PC2')\n",
    "axes[1].set_title(f'K-Means Amélioré: 3 clusters\\n(Silhouette: {silhouette_3:.3f})')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Ajouter les centres des clusters pour le modèle amélioré\n",
    "centers_3_pca = pca_model.transform(scaler.transform(kmeans_3.cluster_centers_))\n",
    "axes[1].scatter(centers_3_pca[:, 0], centers_3_pca[:, 1], \n",
    "               marker='x', s=300, linewidths=4, color='red', label='Centres')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ===================================================================\n",
    "# ANALYSE BUSINESS DES 3 CLUSTERS\n",
    "# ===================================================================\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(\"ANALYSE BUSINESS - 3 SEGMENTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Ajouter les nouveaux clusters aux données\n",
    "filtered_3_clusters = filtered.loc[df_scaled.index].copy()\n",
    "filtered_3_clusters['Segment'] = labels_3\n",
    "\n",
    "# Analyser chaque segment\n",
    "segments_analysis = {}\n",
    "\n",
    "for cluster_id in sorted(set(labels_3)):\n",
    "    cluster_data = filtered_3_clusters[filtered_3_clusters['Segment'] == cluster_id]\n",
    "    \n",
    "    # Calculs statistiques\n",
    "    avg_price = cluster_data['price'].mean() if 'price' in cluster_data.columns else 0\n",
    "    avg_size = cluster_data['size'].mean() if 'size' in cluster_data.columns else 0\n",
    "    count = len(cluster_data)\n",
    "    \n",
    "    # Déterminer le type de segment\n",
    "    price_quartiles = filtered_3_clusters['price'].quantile([0.33, 0.67])\n",
    "    \n",
    "    if avg_price <= price_quartiles[0.33]:\n",
    "        segment_type = \"ÉCONOMIQUE\"\n",
    "        emoji = \"💰\"\n",
    "    elif avg_price <= price_quartiles[0.67]:\n",
    "        segment_type = \"MOYEN\"\n",
    "        emoji = \"🏠\"\n",
    "    else:\n",
    "        segment_type = \"PREMIUM\"\n",
    "        emoji = \"💎\"\n",
    "    \n",
    "    segments_analysis[cluster_id] = {\n",
    "        'type': segment_type,\n",
    "        'emoji': emoji,\n",
    "        'count': count,\n",
    "        'avg_price': avg_price,\n",
    "        'avg_size': avg_size\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{emoji} SEGMENT {cluster_id} - {segment_type}\")\n",
    "    print(f\"   • {count} propriétés ({count/len(filtered_3_clusters)*100:.1f}%)\")\n",
    "    if 'price' in cluster_data.columns:\n",
    "        print(f\"   • Loyer moyen: {avg_price:.0f} TND/mois\")\n",
    "        print(f\"   • Range prix: {cluster_data['price'].min():.0f} - {cluster_data['price'].max():.0f} TND\")\n",
    "    if 'size' in cluster_data.columns:\n",
    "        print(f\"   • Superficie moyenne: {avg_size:.0f} m²\")\n",
    "    \n",
    "    # Quartiers principaux\n",
    "    if 'neighborhood' in cluster_data.columns and not cluster_data['neighborhood'].dropna().empty:\n",
    "        neighborhoods = cluster_data['neighborhood'].value_counts().head(2)\n",
    "        print(f\"   • Quartiers: {neighborhoods.to_dict()}\")\n",
    "    \n",
    "    # Recommandations business\n",
    "    if segment_type == \"ÉCONOMIQUE\":\n",
    "        print(f\"   💡 Cible: Étudiants, jeunes professionnels, budgets serrés\")\n",
    "    elif segment_type == \"MOYEN\":\n",
    "        print(f\"   💡 Cible: Familles, professionnels, marché principal\")\n",
    "    else:\n",
    "        print(f\"   💡 Cible: Cadres, expatriés, clientèle aisée\")\n",
    "\n",
    "# ===================================================================\n",
    "# COMPARAISON FINALE\n",
    "# ===================================================================\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(\"COMPARAISON FINALE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "comparison_final = pd.DataFrame({\n",
    "    'Approche': ['K-Means Auto (8 clusters)', 'K-Means Optimisé (3 clusters)'],\n",
    "    'Score Silhouette': [f\"{kmeans_metrics['silhouette_score']:.4f}\", f\"{silhouette_3:.4f}\"],\n",
    "    'Clusters viables': [f\"{sum(1 for c in cluster_counts.values if c >= 3)}/8\", \"3/3\"],\n",
    "    'Interprétation': ['Difficile (trop fragmenté)', 'Claire (3 segments)'],\n",
    "    'Business': ['Non applicable', 'Applicable']\n",
    "})\n",
    "\n",
    "print(comparison_final.to_string(index=False))\n",
    "\n",
    "# ===================================================================\n",
    "# RECOMMANDATION FINALE\n",
    "# ===================================================================\n",
    "print(f\"\\n🎯 RECOMMANDATION FINALE:\")\n",
    "print(f\"=\"*30)\n",
    "\n",
    "if silhouette_3 >= kmeans_metrics['silhouette_score'] * 0.9:  # Si dans les 90%\n",
    "    print(f\"✅ UTILISER 3 CLUSTERS\")\n",
    "    print(f\"   • Score silhouette acceptable: {silhouette_3:.4f}\")\n",
    "    print(f\"   • Segments business viables\")\n",
    "    print(f\"   • Interprétation claire\")\n",
    "    \n",
    "    # Sauvegarder les meilleurs résultats\n",
    "    final_clusters = labels_3\n",
    "    final_model = kmeans_3\n",
    "    print(f\"\\n💾 Variables créées:\")\n",
    "    print(f\"   • final_clusters: labels des 3 segments\")\n",
    "    print(f\"   • final_model: modèle K-Means optimisé\")\n",
    "    print(f\"   • filtered_3_clusters: données avec segments\")\n",
    "    \n",
    "else:\n",
    "    print(f\"⚠️ CONSIDÉRER D'AUTRES APPROCHES\")\n",
    "    print(f\"   • Essayer DBSCAN ou CAH\")\n",
    "    print(f\"   • Revoir les caractéristiques utilisées\")\n",
    "    print(f\"   • Augmenter le dataset\")\n",
    "\n",
    "print(f\"\\n🎉 ANALYSE TERMINÉE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b890e04",
   "metadata": {},
   "source": [
    "PC1 capture ~50% de la variabilité des données\n",
    "PC2 capture ~12% de la variabilité\n",
    "Total : ~62% de l'information préservée"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f76f2cd",
   "metadata": {},
   "source": [
    "## DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fc82fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_model, dbscan_labels, dbscan_metrics = model_functions.apply_dbscan_clustering(\n",
    "    df_scaled,\n",
    "    eps_range=(2.5, 3),      # Range pour eps\n",
    "    min_samples_range=(2, 18)   # Range pour min_samples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eece522",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualisation DBSCAN avec matplotlib\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Clusters DBSCAN en 2D PCA\n",
    "unique_labels = sorted(set(dbscan_labels))\n",
    "colors_dbscan = plt.cm.Set3(np.linspace(0, 1, len(unique_labels)))\n",
    "\n",
    "for i, label in enumerate(unique_labels):\n",
    "    mask = dbscan_labels == label\n",
    "    if label == -1:\n",
    "        # Points de bruit en noir\n",
    "        axes[0,0].scatter(df_pca.iloc[mask, 0], df_pca.iloc[mask, 1], \n",
    "                         c='black', label='Bruit', alpha=0.7, marker='x')\n",
    "    else:\n",
    "        axes[0,0].scatter(df_pca.iloc[mask, 0], df_pca.iloc[mask, 1], \n",
    "                         c=[colors_dbscan[i]], label=f'Cluster {label}', alpha=0.7)\n",
    "\n",
    "axes[0,0].set_xlabel('PC1')\n",
    "axes[0,0].set_ylabel('PC2')\n",
    "axes[0,0].set_title('DBSCAN - Clusters (2D PCA)')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Distribution des clusters DBSCAN\n",
    "cluster_counts_dbscan = pd.Series(dbscan_labels).value_counts().sort_index()\n",
    "cluster_names = ['Bruit' if idx == -1 else f'Cluster {idx}' for idx in cluster_counts_dbscan.index]\n",
    "bar_colors = ['black' if idx == -1 else colors_dbscan[i] for i, idx in enumerate(cluster_counts_dbscan.index)]\n",
    "\n",
    "bars = axes[0,1].bar(range(len(cluster_counts_dbscan)), cluster_counts_dbscan.values, color=bar_colors)\n",
    "axes[0,1].set_xticks(range(len(cluster_counts_dbscan)))\n",
    "axes[0,1].set_xticklabels(cluster_names, rotation=45)\n",
    "axes[0,1].set_ylabel('Nombre de propriétés')\n",
    "axes[0,1].set_title('Distribution des clusters DBSCAN')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Ajouter les valeurs sur les barres\n",
    "for bar, count in zip(bars, cluster_counts_dbscan.values):\n",
    "    axes[0,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "                   str(count), ha='center', va='bottom')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748d24e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse DBSCAN: \n",
    "print(f\"• Paramètres DBSCAN: eps={dbscan_metrics['eps']:.3f}, min_samples={dbscan_metrics['min_samples']}\")\n",
    "print(f\"• Nombre total de propriétés: {len(dbscan_labels)}\")\n",
    "print(f\"• Nombre de clusters formés: {dbscan_metrics['n_clusters']}\")\n",
    "print(f\"• Points de bruit: {dbscan_metrics['n_noise_points']} ({dbscan_metrics['noise_ratio']*100:.1f}%)\")\n",
    "\n",
    "if dbscan_metrics.get('silhouette_score', 0) > 0:\n",
    "    print(f\"• Score de silhouette: {dbscan_metrics['silhouette_score']:.4f}\")\n",
    "\n",
    "# Analyser la distribution des clusters\n",
    "unique_labels = set(dbscan_labels)\n",
    "cluster_distribution = {}\n",
    "\n",
    "print(f\"\\n📈 DISTRIBUTION DÉTAILLÉE:\")\n",
    "for label in sorted(unique_labels):\n",
    "    count = sum(dbscan_labels == label)\n",
    "    percentage = (count / len(dbscan_labels)) * 100\n",
    "    cluster_distribution[label] = count\n",
    "    \n",
    "    if label == -1:\n",
    "        print(f\"🔍 BRUIT: {count} propriétés ({percentage:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"🏷️ CLUSTER {label}: {count} propriétés ({percentage:.1f}%)\")\n",
    "\n",
    "# ===================================================================\n",
    "# 2. INTERPRÉTATION DE LA QUALITÉ DU CLUSTERING\n",
    "# ===================================================================\n",
    "\n",
    "print(f\"\\n🎯 ÉVALUATION DE LA QUALITÉ:\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Évaluer la qualité selon différents critères\n",
    "noise_ratio = dbscan_metrics['noise_ratio']\n",
    "n_clusters = dbscan_metrics['n_clusters']\n",
    "\n",
    "# Critères de qualité\n",
    "print(f\"\\n📊 CRITÈRES D'ÉVALUATION:\")\n",
    "\n",
    "# 1. Ratio de bruit\n",
    "if noise_ratio < 0.1:\n",
    "    noise_quality = \"EXCELLENT\"\n",
    "    noise_emoji = \"✅\"\n",
    "elif noise_ratio < 0.2:\n",
    "    noise_quality = \"BON\"\n",
    "    noise_emoji = \"👍\"\n",
    "elif noise_ratio < 0.4:\n",
    "    noise_quality = \"ACCEPTABLE\"\n",
    "    noise_emoji = \"⚠️\"\n",
    "else:\n",
    "    noise_quality = \"PROBLÉMATIQUE\"\n",
    "    noise_emoji = \"❌\"\n",
    "\n",
    "print(f\"{noise_emoji} Ratio de bruit: {noise_ratio*100:.1f}% - {noise_quality}\")\n",
    "\n",
    "# 2. Nombre de clusters\n",
    "if n_clusters == 0:\n",
    "    cluster_quality = \"AUCUN CLUSTER\"\n",
    "    cluster_emoji = \"❌\"\n",
    "elif n_clusters == 1:\n",
    "    cluster_quality = \"UN SEUL GROUPE\"\n",
    "    cluster_emoji = \"⚠️\"\n",
    "elif 2 <= n_clusters <= 4:\n",
    "    cluster_quality = \"OPTIMAL\"\n",
    "    cluster_emoji = \"✅\"\n",
    "else:\n",
    "    cluster_quality = \"TROP FRAGMENTÉ\"\n",
    "    cluster_emoji = \"⚠️\"\n",
    "\n",
    "print(f\"{cluster_emoji} Nombre de clusters: {n_clusters} - {cluster_quality}\")\n",
    "\n",
    "# 3. Taille des clusters\n",
    "min_cluster_size = min([count for label, count in cluster_distribution.items() if label != -1], default=0)\n",
    "if min_cluster_size >= 3:\n",
    "    size_quality = \"CLUSTERS VIABLES\"\n",
    "    size_emoji = \"✅\"\n",
    "elif min_cluster_size >= 2:\n",
    "    size_quality = \"CLUSTERS PETITS\"\n",
    "    size_emoji = \"⚠️\"\n",
    "else:\n",
    "    size_quality = \"CLUSTERS TROP PETITS\"\n",
    "    size_emoji = \"❌\"\n",
    "\n",
    "print(f\"{size_emoji} Taille minimale des clusters: {min_cluster_size} - {size_quality}\")\n",
    "\n",
    "# ===================================================================\n",
    "# 3. ANALYSE DÉTAILLÉE PAR CLUSTER\n",
    "# ===================================================================\n",
    "\n",
    "print(f\"\\n🔍 ANALYSE DÉTAILLÉE PAR CLUSTER:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Créer un DataFrame avec les données originales et les clusters\n",
    "filtered_with_dbscan = filtered_3_clusters.copy()\n",
    "filtered_with_dbscan['Cluster_DBSCAN'] = dbscan_labels\n",
    "\n",
    "# Analyser chaque cluster individuellement\n",
    "for label in sorted(unique_labels):\n",
    "    cluster_data = filtered_with_dbscan[filtered_with_dbscan['Cluster_DBSCAN'] == label]\n",
    "    \n",
    "    if label == -1:\n",
    "        print(f\"\\n🔍 ANALYSE DES POINTS DE BRUIT - {len(cluster_data)} propriétés\")\n",
    "        print(\"-\" * 60)\n",
    "        print(\"❓ QUE REPRÉSENTENT CES PROPRIÉTÉS ?\")\n",
    "        print(\"• Propriétés avec des caractéristiques UNIQUES/ATYPIQUES\")\n",
    "        print(\"• Ne ressemblent à aucun autre groupe\")\n",
    "        print(\"• Peuvent être des OPPORTUNITÉS ou des PROBLÈMES\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\n🏷️ CLUSTER {label} - {len(cluster_data)} propriétés\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"🎯 PROFIL DU GROUPE:\")\n",
    "    \n",
    "    # Analyser les caractéristiques du cluster/bruit\n",
    "    if len(cluster_data) > 0:\n",
    "        \n",
    "        # 1. PRIX\n",
    "        if 'price' in cluster_data.columns:\n",
    "            prix_stats = {\n",
    "                'moyenne': cluster_data['price'].mean(),\n",
    "                'mediane': cluster_data['price'].median(), \n",
    "                'min': cluster_data['price'].min(),\n",
    "                'max': cluster_data['price'].max(),\n",
    "                'ecart_type': cluster_data['price'].std()\n",
    "            }\n",
    "            \n",
    "            print(f\"💰 PRIX DE LOCATION:\")\n",
    "            print(f\"   • Moyenne: {prix_stats['moyenne']:.0f} TND/mois\")\n",
    "            print(f\"   • Médiane: {prix_stats['mediane']:.0f} TND/mois\")\n",
    "            print(f\"   • Range: {prix_stats['min']:.0f} - {prix_stats['max']:.0f} TND\")\n",
    "            if len(cluster_data) > 1:\n",
    "                print(f\"   • Écart-type: {prix_stats['ecart_type']:.0f} TND\")\n",
    "            \n",
    "            # Comparer à la moyenne générale\n",
    "            prix_moyen_general = filtered_with_dbscan['price'].mean()\n",
    "            difference = prix_stats['moyenne'] - prix_moyen_general\n",
    "            if abs(difference) > 50:  # Si différence significative\n",
    "                direction = \"PLUS CHER\" if difference > 0 else \"MOINS CHER\"\n",
    "                print(f\"   📊 {direction} que la moyenne générale ({difference:+.0f} TND)\")\n",
    "        \n",
    "        # 2. SUPERFICIE\n",
    "        if 'size' in cluster_data.columns:\n",
    "            size_stats = {\n",
    "                'moyenne': cluster_data['size'].mean(),\n",
    "                'min': cluster_data['size'].min(),\n",
    "                'max': cluster_data['size'].max()\n",
    "            }\n",
    "            \n",
    "            print(f\"📐 SUPERFICIE:\")\n",
    "            print(f\"   • Moyenne: {size_stats['moyenne']:.0f} m²\")\n",
    "            print(f\"   • Range: {size_stats['min']:.0f} - {size_stats['max']:.0f} m²\")\n",
    "        \n",
    "        # 3. NOMBRE DE PIÈCES\n",
    "        if 'rooms' in cluster_data.columns:\n",
    "            rooms_avg = cluster_data['rooms'].mean()\n",
    "            print(f\"🏠 PIÈCES: {rooms_avg:.1f} en moyenne\")\n",
    "        \n",
    "        # 4. LOCALISATION\n",
    "        if 'neighborhood' in cluster_data.columns and not cluster_data['neighborhood'].dropna().empty:\n",
    "            quartiers = cluster_data['neighborhood'].value_counts()\n",
    "            print(f\"🏘️ QUARTIERS PRINCIPAUX:\")\n",
    "            for quartier, count in quartiers.head(3).items():\n",
    "                percentage = (count / len(cluster_data)) * 100\n",
    "                print(f\"   • {quartier}: {count} propriétés ({percentage:.0f}%)\")\n",
    "        \n",
    "        # 5. ÉTAT DES PROPRIÉTÉS\n",
    "        if 'condition' in cluster_data.columns and not cluster_data['condition'].dropna().empty:\n",
    "            conditions = cluster_data['condition'].value_counts()\n",
    "            print(f\"🔧 ÉTAT DES PROPRIÉTÉS:\")\n",
    "            for condition, count in conditions.head(3).items():\n",
    "                percentage = (count / len(cluster_data)) * 100\n",
    "                print(f\"   • {condition}: {count} propriétés ({percentage:.0f}%)\")\n",
    "        \n",
    "        # 6. FINITIONS\n",
    "        if 'finishing' in cluster_data.columns and not cluster_data['finishing'].dropna().empty:\n",
    "            finitions = cluster_data['finishing'].value_counts()\n",
    "            print(f\"✨ FINITIONS:\")\n",
    "            for finition, count in finitions.head(3).items():\n",
    "                percentage = (count / len(cluster_data)) * 100\n",
    "                print(f\"   • {finition}: {count} propriétés ({percentage:.0f}%)\")\n",
    "        \n",
    "        # 7. ÉQUIPEMENTS (si disponibles)\n",
    "        equipment_cols = ['air_conditioning', 'elevator', 'garden', 'equipped_kitchen', 'swimming_pool']\n",
    "        available_equipment = [col for col in equipment_cols if col in cluster_data.columns]\n",
    "        \n",
    "        if available_equipment:\n",
    "            print(f\"⚡ ÉQUIPEMENTS:\")\n",
    "            for equip in available_equipment:\n",
    "                if cluster_data[equip].dtype in ['int64', 'float64']:\n",
    "                    percentage = (cluster_data[equip] == 1).mean() * 100\n",
    "                    if percentage > 0:\n",
    "                        print(f\"   • {equip}: {percentage:.0f}% des propriétés\")\n",
    "\n",
    "# ===================================================================\n",
    "# 4. INTERPRÉTATION BUSINESS AVANCÉE\n",
    "# ===================================================================\n",
    "\n",
    "print(f\"\\n💼 INTERPRÉTATION BUSINESS:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Analyser les segments de marché identifiés\n",
    "segments_business = {}\n",
    "\n",
    "for label in sorted(unique_labels):\n",
    "    if label == -1:\n",
    "        continue  # On analyse le bruit séparément\n",
    "    \n",
    "    cluster_data = filtered_with_dbscan[filtered_with_dbscan['Cluster_DBSCAN'] == label]\n",
    "    \n",
    "    if 'price' in cluster_data.columns and len(cluster_data) > 0:\n",
    "        avg_price = cluster_data['price'].mean()\n",
    "        avg_size = cluster_data['size'].mean() if 'size' in cluster_data.columns else 0\n",
    "        \n",
    "        # Déterminer le segment de marché\n",
    "        price_quartiles = filtered_with_dbscan['price'].quantile([0.25, 0.5, 0.75])\n",
    "        \n",
    "        if avg_price <= price_quartiles[0.25]:\n",
    "            segment_type = \"ÉCONOMIQUE\"\n",
    "            segment_emoji = \"💰\"\n",
    "            target = \"Étudiants, jeunes professionnels, budgets serrés\"\n",
    "            strategy = \"Prix compétitifs, localisation accessible\"\n",
    "        elif avg_price <= price_quartiles[0.5]:\n",
    "            segment_type = \"ENTRÉE DE GAMME\"\n",
    "            segment_emoji = \"🏠\"\n",
    "            target = \"Primo-locataires, petites familles\"\n",
    "            strategy = \"Bon rapport qualité/prix, commodités de base\"\n",
    "        elif avg_price <= price_quartiles[0.75]:\n",
    "            segment_type = \"MOYEN STANDING\"\n",
    "            segment_emoji = \"🏡\"\n",
    "            target = \"Familles, professionnels établis\"\n",
    "            strategy = \"Confort, équipements, quartiers résidentiels\"\n",
    "        else:\n",
    "            segment_type = \"HAUT DE GAMME\"\n",
    "            segment_emoji = \"💎\"\n",
    "            target = \"Cadres, expatriés, clientèle aisée\"\n",
    "            strategy = \"Luxe, services premium, localisations privilégiées\"\n",
    "        \n",
    "        segments_business[label] = {\n",
    "            'type': segment_type,\n",
    "            'emoji': segment_emoji,\n",
    "            'count': len(cluster_data),\n",
    "            'avg_price': avg_price,\n",
    "            'avg_size': avg_size,\n",
    "            'target': target,\n",
    "            'strategy': strategy\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{segment_emoji} CLUSTER {label}: SEGMENT {segment_type}\")\n",
    "        print(f\"   📊 {len(cluster_data)} propriétés ({len(cluster_data)/len(filtered_with_dbscan)*100:.1f}% du marché)\")\n",
    "        print(f\"   💰 Prix moyen: {avg_price:.0f} TND/mois\")\n",
    "        if avg_size > 0:\n",
    "            print(f\"   📐 Superficie moyenne: {avg_size:.0f} m²\")\n",
    "        print(f\"   🎯 Cible: {target}\")\n",
    "        print(f\"   💡 Stratégie: {strategy}\")\n",
    "\n",
    "# Analyser les points de bruit spécifiquement\n",
    "bruit_data = filtered_with_dbscan[filtered_with_dbscan['Cluster_DBSCAN'] == -1]\n",
    "\n",
    "if len(bruit_data) > 0:\n",
    "    print(f\"\\n🔍 ANALYSE SPÉCIALE: POINTS DE BRUIT\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"🎯 OPPORTUNITÉS POTENTIELLES:\")\n",
    "    \n",
    "    if 'price' in bruit_data.columns:\n",
    "        prix_bruit = bruit_data['price'].tolist()\n",
    "        prix_median_general = filtered_with_dbscan['price'].median()\n",
    "        \n",
    "        print(f\"💰 Prix des propriétés atypiques: {prix_bruit}\")\n",
    "        print(f\"📊 Prix médian général: {prix_median_general:.0f} TND\")\n",
    "        \n",
    "        # Identifier les bonnes affaires potentielles\n",
    "        bonnes_affaires = bruit_data[bruit_data['price'] < prix_median_general]\n",
    "        proprietes_cheres = bruit_data[bruit_data['price'] > prix_median_general]\n",
    "        \n",
    "        if len(bonnes_affaires) > 0:\n",
    "            print(f\"✅ BONNES AFFAIRES POTENTIELLES: {len(bonnes_affaires)} propriétés\")\n",
    "            print(f\"   • Prix sous la médiane du marché\")\n",
    "            print(f\"   • À investiguer: raison du prix bas?\")\n",
    "        \n",
    "        if len(proprietes_cheres) > 0:\n",
    "            print(f\"💎 PROPRIÉTÉS PREMIUM UNIQUES: {len(proprietes_cheres)} propriétés\")\n",
    "            print(f\"   • Caractéristiques exceptionnelles\")\n",
    "            print(f\"   • Segments de niche\")\n",
    "\n",
    "# ===================================================================\n",
    "# 5. RECOMMANDATIONS D'ACTIONS\n",
    "# ===================================================================\n",
    "\n",
    "print(f\"\\n🚀 RECOMMANDATIONS D'ACTIONS:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"\\n📈 STRATÉGIES PAR CLUSTER:\")\n",
    "for label, info in segments_business.items():\n",
    "    print(f\"\\n{info['emoji']} CLUSTER {label} ({info['type']}):\")\n",
    "    print(f\"   📊 Marketing: Cibler {info['target'].lower()}\")\n",
    "    print(f\"   💡 Positionnement: {info['strategy']}\")\n",
    "    \n",
    "    # Recommandations spécifiques selon la taille du cluster\n",
    "    if info['count'] >= 3:\n",
    "        print(f\"   ✅ Segment viable: développer l'offre\")\n",
    "    else:\n",
    "        print(f\"   ⚠️ Petit segment: analyser la demande\")\n",
    "\n",
    "print(f\"\\n🔍 ACTIONS POUR LES POINTS DE BRUIT:\")\n",
    "if len(bruit_data) > 0:\n",
    "    print(f\"• Vérifier la qualité des données\")\n",
    "    print(f\"• Investiguer les raisons de l'atypisme\")\n",
    "    print(f\"• Identifier les opportunités de niche\")\n",
    "    print(f\"• Contrôler les prix aberrants\")\n",
    "else:\n",
    "    print(f\"• Aucun point de bruit détecté\")\n",
    "    print(f\"• Marché homogène et prévisible\")\n",
    "\n",
    "# ===================================================================\n",
    "# 6. CONCLUSION GÉNÉRALE\n",
    "# ===================================================================\n",
    "\n",
    "print(f\"\\n🎯 CONCLUSION GÉNÉRALE:\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "if n_clusters >= 2 and noise_ratio < 0.3:\n",
    "    print(f\"✅ DBSCAN A RÉUSSI À SEGMENTER VOS DONNÉES\")\n",
    "    print(f\"   • {n_clusters} segments naturels identifiés\")\n",
    "    print(f\"   • {dbscan_metrics['n_noise_points']} propriétés atypiques détectées\")\n",
    "    print(f\"   • Segmentation applicable pour le business\")\n",
    "    \n",
    "    # Identifier le segment principal\n",
    "    main_cluster = max(segments_business.items(), key=lambda x: x[1]['count'])\n",
    "    print(f\"\\n🎖️ SEGMENT PRINCIPAL: {main_cluster[1]['type']}\")\n",
    "    print(f\"   • {main_cluster[1]['count']} propriétés\")\n",
    "    print(f\"   • Représente le cœur de votre marché\")\n",
    "\n",
    "elif n_clusters == 1:\n",
    "    print(f\"⚠️ MARCHÉ TRÈS HOMOGÈNE\")\n",
    "    print(f\"   • Un seul groupe principal détecté\")\n",
    "    print(f\"   • Peu de différenciation dans l'offre\")\n",
    "    print(f\"   • Opportunité de créer des niches\")\n",
    "\n",
    "elif n_clusters == 0:\n",
    "    print(f\"❌ DONNÉES TROP DISPERSÉES\")\n",
    "    print(f\"   • DBSCAN n'a trouvé aucun cluster\")\n",
    "    print(f\"   • Tous les points considérés comme du bruit\")\n",
    "    print(f\"   • Recommandation: utiliser K-Means\")\n",
    "\n",
    "else:\n",
    "    print(f\"⚠️ SEGMENTATION COMPLEXE\")\n",
    "    print(f\"   • {n_clusters} clusters détectés\")\n",
    "    print(f\"   • Marché très fragmenté\")\n",
    "    print(f\"   • Analyser la viabilité de chaque segment\")\n",
    "\n",
    "print(f\"\\n💾 DONNÉES DISPONIBLES:\")\n",
    "print(f\"• filtered_with_dbscan: DataFrame avec clusters DBSCAN\")\n",
    "print(f\"• dbscan_labels: labels des clusters pour chaque propriété\")\n",
    "print(f\"• dbscan_metrics: métriques de performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b77984c",
   "metadata": {},
   "source": [
    "## CAH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d024c48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAH avec méthode Ward pour les appartements mis à la vente à La Soukra  (>150 observations)\n",
    "\n",
    "filtered_df = df_scaled[(df['transaction'] == 'sale') & (df['city'] == 'La Soukra') & (df['property_type'] == 'appartement')]\n",
    "# 1 -  CAH\n",
    "linkage_matrix, cluster_labels, optimal_n_clusters, metrics = model_functions.apply_cah_clustering(\n",
    "    filtered_df, \n",
    "    max_clusters=10, \n",
    "    linkage_method='ward'\n",
    ")\n",
    "\n",
    "# 2  dendrogramme\n",
    "print(\"\\n📊 DENDROGRAMME:\")\n",
    "dendrogram_data = model_functions.visualize_cah_dendrogram(\n",
    "    linkage_matrix, \n",
    "    optimal_n_clusters=optimal_n_clusters,\n",
    "    max_display=100\n",
    ")\n",
    "\n",
    "# 3. analyse complète\n",
    "results_df, detailed_df, hierarchy_df = model_functions.complete_cah_analysis_after_dendrogram(\n",
    "    df, filtered_df, linkage_matrix, cluster_labels, optimal_n_clusters, feature_names\n",
    ")\n",
    "print(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
